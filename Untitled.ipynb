{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import OrdinalEncoder\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv(\"D:\\Data scientist\\Dataset\\data_breast_cancer.csv\",header=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>'60-69'</td>\n",
       "      <td>'ge40'</td>\n",
       "      <td>'10-14'</td>\n",
       "      <td>'0-2'</td>\n",
       "      <td>'no'</td>\n",
       "      <td>'2'</td>\n",
       "      <td>'right'</td>\n",
       "      <td>'ge40'</td>\n",
       "      <td>'yes'</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>'60-69'</td>\n",
       "      <td>'ge40'</td>\n",
       "      <td>'25-29'</td>\n",
       "      <td>'0-2'</td>\n",
       "      <td>'no'</td>\n",
       "      <td>'3'</td>\n",
       "      <td>'left'</td>\n",
       "      <td>'ge40'</td>\n",
       "      <td>'yes'</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>'40-49'</td>\n",
       "      <td>'premeno'</td>\n",
       "      <td>'20-24'</td>\n",
       "      <td>'0-2'</td>\n",
       "      <td>'no'</td>\n",
       "      <td>'2'</td>\n",
       "      <td>'right'</td>\n",
       "      <td>'premeno'</td>\n",
       "      <td>'no'</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>'40-49'</td>\n",
       "      <td>'premeno'</td>\n",
       "      <td>'20-24'</td>\n",
       "      <td>'0-2'</td>\n",
       "      <td>'no'</td>\n",
       "      <td>'2'</td>\n",
       "      <td>'left'</td>\n",
       "      <td>'premeno'</td>\n",
       "      <td>'no'</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>'50-59'</td>\n",
       "      <td>'ge40'</td>\n",
       "      <td>'35-39'</td>\n",
       "      <td>'0-2'</td>\n",
       "      <td>'no'</td>\n",
       "      <td>'2'</td>\n",
       "      <td>'left'</td>\n",
       "      <td>'ge40'</td>\n",
       "      <td>'no'</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>223</th>\n",
       "      <td>'50-59'</td>\n",
       "      <td>'ge40'</td>\n",
       "      <td>'10-14'</td>\n",
       "      <td>'0-2'</td>\n",
       "      <td>'no'</td>\n",
       "      <td>'2'</td>\n",
       "      <td>'left'</td>\n",
       "      <td>'ge40'</td>\n",
       "      <td>'no'</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>224</th>\n",
       "      <td>'60-69'</td>\n",
       "      <td>'ge40'</td>\n",
       "      <td>'35-39'</td>\n",
       "      <td>'6-8'</td>\n",
       "      <td>'yes'</td>\n",
       "      <td>'3'</td>\n",
       "      <td>'left'</td>\n",
       "      <td>'ge40'</td>\n",
       "      <td>'no'</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>225</th>\n",
       "      <td>'60-69'</td>\n",
       "      <td>'ge40'</td>\n",
       "      <td>'10-14'</td>\n",
       "      <td>'0-2'</td>\n",
       "      <td>'no'</td>\n",
       "      <td>'1'</td>\n",
       "      <td>'left'</td>\n",
       "      <td>'ge40'</td>\n",
       "      <td>'no'</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>226</th>\n",
       "      <td>'40-49'</td>\n",
       "      <td>'premeno'</td>\n",
       "      <td>'35-39'</td>\n",
       "      <td>'0-2'</td>\n",
       "      <td>'yes'</td>\n",
       "      <td>'3'</td>\n",
       "      <td>'right'</td>\n",
       "      <td>'premeno'</td>\n",
       "      <td>'yes'</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>227</th>\n",
       "      <td>'40-49'</td>\n",
       "      <td>'ge40'</td>\n",
       "      <td>'25-29'</td>\n",
       "      <td>'0-2'</td>\n",
       "      <td>'no'</td>\n",
       "      <td>'2'</td>\n",
       "      <td>'left'</td>\n",
       "      <td>'ge40'</td>\n",
       "      <td>'no'</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>228 rows Ã— 9 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "           0          1        2      3      4    5        6          7      8\n",
       "0    '60-69'     'ge40'  '10-14'  '0-2'   'no'  '2'  'right'     'ge40'  'yes'\n",
       "1    '60-69'     'ge40'  '25-29'  '0-2'   'no'  '3'   'left'     'ge40'  'yes'\n",
       "2    '40-49'  'premeno'  '20-24'  '0-2'   'no'  '2'  'right'  'premeno'   'no'\n",
       "3    '40-49'  'premeno'  '20-24'  '0-2'   'no'  '2'   'left'  'premeno'   'no'\n",
       "4    '50-59'     'ge40'  '35-39'  '0-2'   'no'  '2'   'left'     'ge40'   'no'\n",
       "..       ...        ...      ...    ...    ...  ...      ...        ...    ...\n",
       "223  '50-59'     'ge40'  '10-14'  '0-2'   'no'  '2'   'left'     'ge40'   'no'\n",
       "224  '60-69'     'ge40'  '35-39'  '6-8'  'yes'  '3'   'left'     'ge40'   'no'\n",
       "225  '60-69'     'ge40'  '10-14'  '0-2'   'no'  '1'   'left'     'ge40'   'no'\n",
       "226  '40-49'  'premeno'  '35-39'  '0-2'  'yes'  '3'  'right'  'premeno'  'yes'\n",
       "227  '40-49'     'ge40'  '25-29'  '0-2'   'no'  '2'   'left'     'ge40'   'no'\n",
       "\n",
       "[228 rows x 9 columns]"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_traindf = pd.DataFrame(x_train)\n",
    "x_traindf\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>4.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>4.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>3.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>223</th>\n",
       "      <td>3.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>224</th>\n",
       "      <td>4.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>225</th>\n",
       "      <td>4.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>226</th>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>227</th>\n",
       "      <td>2.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>228 rows Ã— 9 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       0    1    2    3    4    5    6    7    8\n",
       "0    4.0  0.0  1.0  0.0  0.0  1.0  1.0  0.0  1.0\n",
       "1    4.0  0.0  4.0  0.0  0.0  2.0  0.0  0.0  1.0\n",
       "2    2.0  2.0  3.0  0.0  0.0  1.0  1.0  2.0  0.0\n",
       "3    2.0  2.0  3.0  0.0  0.0  1.0  0.0  2.0  0.0\n",
       "4    3.0  0.0  6.0  0.0  0.0  1.0  0.0  0.0  0.0\n",
       "..   ...  ...  ...  ...  ...  ...  ...  ...  ...\n",
       "223  3.0  0.0  1.0  0.0  0.0  1.0  0.0  0.0  0.0\n",
       "224  4.0  0.0  6.0  5.0  1.0  2.0  0.0  0.0  0.0\n",
       "225  4.0  0.0  1.0  0.0  0.0  0.0  0.0  0.0  0.0\n",
       "226  2.0  2.0  6.0  0.0  1.0  2.0  1.0  2.0  1.0\n",
       "227  2.0  0.0  4.0  0.0  0.0  1.0  0.0  0.0  0.0\n",
       "\n",
       "[228 rows x 9 columns]"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train_encdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    0\n",
       "1    0\n",
       "2    0\n",
       "3    0\n",
       "4    0\n",
       "5    0\n",
       "6    0\n",
       "7    0\n",
       "8    0\n",
       "9    0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.isnull()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "data[4] = data[4].fillna(data[4].mode)\n",
    "data[7] = data[1].fillna(data[1].mode)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = data.iloc[:,:-1].values\n",
    "y = data.iloc[:,-1].values\n",
    "x = x.astype(str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train, x_test, y_train, y_test = train_test_split(x,y,random_state=0,test_size=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "encode = OrdinalEncoder()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "encode.fit(x_train)\n",
    "x_train_enc = encode.transform(x_train)\n",
    "x_test_enc = encode.transform(x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "le = LabelEncoder()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "le.fit(y_train)\n",
    "y_train_enc = le.transform(y_train)\n",
    "y_test_enc = le.transform(y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>4.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>4.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>3.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>223</th>\n",
       "      <td>3.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>224</th>\n",
       "      <td>4.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>225</th>\n",
       "      <td>4.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>226</th>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>227</th>\n",
       "      <td>2.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>228 rows Ã— 9 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       0    1    2    3    4    5    6    7    8\n",
       "0    4.0  0.0  1.0  0.0  0.0  1.0  1.0  0.0  1.0\n",
       "1    4.0  0.0  4.0  0.0  0.0  2.0  0.0  0.0  1.0\n",
       "2    2.0  2.0  3.0  0.0  0.0  1.0  1.0  2.0  0.0\n",
       "3    2.0  2.0  3.0  0.0  0.0  1.0  0.0  2.0  0.0\n",
       "4    3.0  0.0  6.0  0.0  0.0  1.0  0.0  0.0  0.0\n",
       "..   ...  ...  ...  ...  ...  ...  ...  ...  ...\n",
       "223  3.0  0.0  1.0  0.0  0.0  1.0  0.0  0.0  0.0\n",
       "224  4.0  0.0  6.0  5.0  1.0  2.0  0.0  0.0  0.0\n",
       "225  4.0  0.0  1.0  0.0  0.0  0.0  0.0  0.0  0.0\n",
       "226  2.0  2.0  6.0  0.0  1.0  2.0  1.0  2.0  1.0\n",
       "227  2.0  0.0  4.0  0.0  0.0  1.0  0.0  0.0  0.0\n",
       "\n",
       "[228 rows x 9 columns]"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train_encdf = pd.DataFrame(x_train_enc)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "from sklearn.feature_selection import SelectKBest\n",
    "from sklearn.feature_selection import f_regression\n",
    "import statsmodels.api as sm\n",
    "\n",
    "%matplotlib inline\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "pd.set_option('display.max_rows', 2000)\n",
    "pd.set_option('display.max_columns', 500)\n",
    "pd.set_option('display.width', 1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_train = pd.read_csv(\"D:\\Data scientist\\Dataset\\house-prices-advanced-regression-techniques\\house_train.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_test = pd.read_csv(\"D:\\Data scientist\\Dataset\\house-prices-advanced-regression-techniques\\house_test.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.concat([data_train,data_test],axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['LotFrontage'] = data['LotFrontage'].fillna(data['LotFrontage'].mean())\n",
    "data.drop([\"Alley\",\"PoolQC\",\"Fence\",\"MiscFeature\",\"FireplaceQu\"],axis=1,inplace=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['GarageYrBlt'] = data['GarageYrBlt'].fillna(data['GarageYrBlt'].mean())\n",
    "data['GarageType'] = data['GarageType'].fillna(data['GarageType'].mode()[0])\n",
    "data['GarageFinish'] = data['GarageFinish'].fillna(data['GarageFinish'].mode()[0])\n",
    "data['GarageQual'] = data['GarageQual'].fillna(data['GarageQual'].mode()[0])\n",
    "data['GarageCond'] = data['GarageCond'].fillna(data['GarageCond'].mode()[0])\n",
    "data['BsmtQual'] = data['BsmtQual'].fillna(data['BsmtQual'].mode()[0])\n",
    "data['BsmtCond'] = data['BsmtCond'].fillna(data['BsmtCond'].mode()[0])\n",
    "data['BsmtExposure'] = data['BsmtExposure'].fillna(data['BsmtExposure'].mode()[0])\n",
    "data['BsmtFinType1'] = data['BsmtFinType1'].fillna(data['BsmtFinType1'].mode()[0])\n",
    "data['BsmtFinType2'] = data['BsmtFinType2'].fillna(data['BsmtFinType2'].mode()[0])\n",
    "\n",
    "data['MasVnrType'] = data['MasVnrType'].fillna(data['MasVnrType'].mode()[0])\n",
    "data['MasVnrArea'] = data['MasVnrArea'].fillna(data['MasVnrArea'].mean())\n",
    "data['MSZoning'] = data['MSZoning'].fillna(data['MSZoning'].mode()[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "38"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "columns1 = [feature for feature in data.columns if data[feature].dtypes == \"O\"]\n",
    "len(columns1) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_trial = data.copy() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "def category_one_hot_encode(columns_list):\n",
    "    trial_data = data_trial\n",
    "    for feature in columns_list:\n",
    "        \n",
    "        feat_dummy= pd.get_dummies(trial_data[feature],drop_first=True)\n",
    "        trial_data = pd.concat([trial_data,feat_dummy],axis=1)\n",
    "        trial_data.drop([feature],axis=1,inplace=True)\n",
    "    return trial_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "df=category_one_hot_encode(columns1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.drop(['Id'],inplace=True,axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2919, 177)"
      ]
     },
     "execution_count": 92,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.shape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "df=df.loc[:,~df.columns.duplicated()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_train = df.iloc[:1460,:]\n",
    "new_test = df.iloc[1460:,:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "count of Numerical variables 37\n"
     ]
    }
   ],
   "source": [
    "numerical_features = [feature for feature in data_train.columns if data_train[feature].dtypes != 'O' and feature not in ['Id']]\n",
    "\n",
    "print(\"count of Numerical variables\" , len(numerical_features))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "36"
      ]
     },
     "execution_count": 136,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "numerical_features.pop()\n",
    "len(numerical_features)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [],
   "source": [
    "num = numerical_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For correlation\n",
    "new_train_corr = df.iloc[:1460,:]\n",
    "X = new_train_corr[num]\n",
    "Y = new_train_corr['SalePrice']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_features = SelectKBest(score_func =f_regression , k=20 )\n",
    "fit = best_features.fit(X,Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfscore= pd.DataFrame(fit.scores_)\n",
    "dfcolumns=pd.DataFrame(X.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [],
   "source": [
    "featurescore = pd.concat([dfscore,dfcolumns],axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [],
   "source": [
    "featurescore.columns = ['scores','specs']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [],
   "source": [
    "featurescore2  = featurescore.nlargest(18,'scores')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_feat = featurescore2.iloc[:,-1].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['OverallQual', 'GrLivArea', 'GarageCars', 'GarageArea',\n",
       "       'TotalBsmtSF', '1stFlrSF', 'FullBath', 'TotRmsAbvGrd', 'YearBuilt',\n",
       "       'YearRemodAdd', 'MasVnrArea', 'GarageYrBlt', 'Fireplaces',\n",
       "       'BsmtFinSF1', 'LotFrontage', 'WoodDeckSF', '2ndFlrSF',\n",
       "       'OpenPorchSF'], dtype=object)"
      ]
     },
     "execution_count": 145,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "num_feat\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [],
   "source": [
    "f = []\n",
    "for feature in num:\n",
    "    if feature in num_feat:\n",
    "        pass\n",
    "    else:\n",
    "        f.append(feature)\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['MSSubClass',\n",
       " 'LotArea',\n",
       " 'OverallCond',\n",
       " 'BsmtFinSF2',\n",
       " 'BsmtUnfSF',\n",
       " 'LowQualFinSF',\n",
       " 'BsmtFullBath',\n",
       " 'BsmtHalfBath',\n",
       " 'HalfBath',\n",
       " 'BedroomAbvGr',\n",
       " 'KitchenAbvGr',\n",
       " 'EnclosedPorch',\n",
       " '3SsnPorch',\n",
       " 'ScreenPorch',\n",
       " 'PoolArea',\n",
       " 'MiscVal',\n",
       " 'MoSold',\n",
       " 'YrSold']"
      ]
     },
     "execution_count": 147,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "f"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MinMaxScaler(copy=True, feature_range=(0, 1))"
      ]
     },
     "execution_count": 148,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scaler_normalise=MinMaxScaler()\n",
    "#stadscalr  = StandardScaler()\n",
    "scaler_normalise.fit(new_train[num_feat])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Bhavesh\\Miniconda3\\lib\\site-packages\\ipykernel_launcher.py:1: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  \"\"\"Entry point for launching an IPython kernel.\n",
      "C:\\Users\\Bhavesh\\Miniconda3\\lib\\site-packages\\pandas\\core\\indexing.py:966: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  self.obj[item] = s\n",
      "C:\\Users\\Bhavesh\\Miniconda3\\lib\\site-packages\\ipykernel_launcher.py:2: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  \n",
      "C:\\Users\\Bhavesh\\Miniconda3\\lib\\site-packages\\pandas\\core\\indexing.py:966: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  self.obj[item] = s\n"
     ]
    }
   ],
   "source": [
    "new_train[num_feat] = scaler_normalise.transform(new_train[num_feat])\n",
    "new_test[num_feat] = scaler_normalise.transform(new_test[num_feat])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1460, 177)"
      ]
     },
     "execution_count": 150,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [],
   "source": [
    "data1 = pd.concat([new_train,new_test],axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {},
   "outputs": [],
   "source": [
    "data1.drop(['MSSubClass',\n",
    " 'LotArea',\n",
    " 'OverallCond',\n",
    " 'BsmtFinSF2',\n",
    " 'BsmtUnfSF',\n",
    " 'LowQualFinSF',\n",
    " 'BsmtFullBath',\n",
    " 'BsmtHalfBath',\n",
    " 'HalfBath',\n",
    " 'BedroomAbvGr',\n",
    " 'KitchenAbvGr',\n",
    " 'EnclosedPorch',\n",
    " '3SsnPorch',\n",
    " 'ScreenPorch',\n",
    " 'PoolArea',\n",
    " 'MiscVal',\n",
    " 'MoSold',\n",
    " 'YrSold'],axis=1,inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_train = data1.iloc[:1460,:]\n",
    "new_test = data1.iloc[1460:,:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = new_train.drop(['SalePrice'],axis=1)\n",
    "Y = new_train['SalePrice']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_test.drop(['SalePrice'],axis=1,inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os \n",
    "os.environ['KERAS_BACKEND'] = 'theano'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using Theano backend.\n"
     ]
    }
   ],
   "source": [
    "import keras\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.layers import LeakyReLU, PReLU,ELU\n",
    "from keras.layers import Dropout\n",
    "\n",
    "\n",
    "\n",
    "#from keras.wrappers.scikit_learn import KerasClassifier\n",
    "#from keras.activations import relu, sigmoid\n",
    "#from sklearn.model_selection import GridSearchCV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Bhavesh\\Miniconda3\\lib\\site-packages\\ipykernel_launcher.py:3: UserWarning: Update your `Dense` call to the Keras 2 API: `Dense(activation=\"relu\", input_dim=158, units=50, kernel_initializer=\"he_uniform\")`\n",
      "  This is separate from the ipykernel package so we can avoid doing imports until\n",
      "C:\\Users\\Bhavesh\\Miniconda3\\lib\\site-packages\\ipykernel_launcher.py:5: UserWarning: Update your `Dense` call to the Keras 2 API: `Dense(activation=\"relu\", units=25, kernel_initializer=\"he_uniform\")`\n",
      "  \"\"\"\n",
      "C:\\Users\\Bhavesh\\Miniconda3\\lib\\site-packages\\ipykernel_launcher.py:7: UserWarning: Update your `Dense` call to the Keras 2 API: `Dense(activation=\"relu\", units=50, kernel_initializer=\"he_uniform\")`\n",
      "  import sys\n",
      "C:\\Users\\Bhavesh\\Miniconda3\\lib\\site-packages\\ipykernel_launcher.py:9: UserWarning: Update your `Dense` call to the Keras 2 API: `Dense(units=1, kernel_initializer=\"he_uniform\")`\n",
      "  if __name__ == '__main__':\n",
      "C:\\Users\\Bhavesh\\Miniconda3\\lib\\site-packages\\ipykernel_launcher.py:13: UserWarning: The `nb_epoch` argument in `fit` has been renamed `epochs`.\n",
      "  del sys.path[0]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 1168 samples, validate on 292 samples\n",
      "Epoch 1/1000\n",
      "1168/1168 [==============================] - 1s 522us/step - loss: 193749.5836 - val_loss: 196027.7336\n",
      "Epoch 2/1000\n",
      "1168/1168 [==============================] - 0s 126us/step - loss: 191624.2186 - val_loss: 188982.2556\n",
      "Epoch 3/1000\n",
      "1168/1168 [==============================] - 0s 165us/step - loss: 174970.8497 - val_loss: 158118.6318\n",
      "Epoch 4/1000\n",
      "1168/1168 [==============================] - 0s 156us/step - loss: 122121.1005 - val_loss: 83310.4549\n",
      "Epoch 5/1000\n",
      "1168/1168 [==============================] - 0s 124us/step - loss: 65266.0949 - val_loss: 64298.5944\n",
      "Epoch 6/1000\n",
      "1168/1168 [==============================] - 0s 131us/step - loss: 59342.2688 - val_loss: 62136.8174\n",
      "Epoch 7/1000\n",
      "1168/1168 [==============================] - 0s 127us/step - loss: 57394.4281 - val_loss: 60221.0292\n",
      "Epoch 8/1000\n",
      "1168/1168 [==============================] - 0s 140us/step - loss: 55793.6926 - val_loss: 58363.2779\n",
      "Epoch 9/1000\n",
      "1168/1168 [==============================] - 0s 151us/step - loss: 53951.6931 - val_loss: 56560.3665\n",
      "Epoch 10/1000\n",
      "1168/1168 [==============================] - 0s 143us/step - loss: 52345.9165 - val_loss: 54854.5786\n",
      "Epoch 11/1000\n",
      "1168/1168 [==============================] - 0s 135us/step - loss: 50478.6005 - val_loss: 53281.2326\n",
      "Epoch 12/1000\n",
      "1168/1168 [==============================] - 0s 128us/step - loss: 48191.7873 - val_loss: 51884.2084\n",
      "Epoch 13/1000\n",
      "1168/1168 [==============================] - 0s 124us/step - loss: 47050.5420 - val_loss: 50415.5929\n",
      "Epoch 14/1000\n",
      "1168/1168 [==============================] - 0s 129us/step - loss: 45860.8288 - val_loss: 49100.6689\n",
      "Epoch 15/1000\n",
      "1168/1168 [==============================] - 0s 190us/step - loss: 44047.4649 - val_loss: 47933.7625\n",
      "Epoch 16/1000\n",
      "1168/1168 [==============================] - 0s 149us/step - loss: 42053.4025 - val_loss: 47015.3840\n",
      "Epoch 17/1000\n",
      "1168/1168 [==============================] - 0s 166us/step - loss: 41822.8030 - val_loss: 46210.6663\n",
      "Epoch 18/1000\n",
      "1168/1168 [==============================] - 0s 177us/step - loss: 40418.0883 - val_loss: 45218.0471\n",
      "Epoch 19/1000\n",
      "1168/1168 [==============================] - 0s 144us/step - loss: 39547.8346 - val_loss: 44376.5555\n",
      "Epoch 20/1000\n",
      "1168/1168 [==============================] - 0s 185us/step - loss: 39131.3152 - val_loss: 43628.7188\n",
      "Epoch 21/1000\n",
      "1168/1168 [==============================] - 0s 143us/step - loss: 38558.1778 - val_loss: 42999.1689\n",
      "Epoch 22/1000\n",
      "1168/1168 [==============================] - 0s 138us/step - loss: 37479.2297 - val_loss: 42332.7006\n",
      "Epoch 23/1000\n",
      "1168/1168 [==============================] - 0s 146us/step - loss: 36864.7822 - val_loss: 41730.2637\n",
      "Epoch 24/1000\n",
      "1168/1168 [==============================] - 0s 155us/step - loss: 36323.8217 - val_loss: 41129.4505\n",
      "Epoch 25/1000\n",
      "1168/1168 [==============================] - 0s 194us/step - loss: 35343.1526 - val_loss: 40544.5933\n",
      "Epoch 26/1000\n",
      "1168/1168 [==============================] - 0s 191us/step - loss: 34919.3777 - val_loss: 39956.6184\n",
      "Epoch 27/1000\n",
      "1168/1168 [==============================] - 0s 138us/step - loss: 34282.8752 - val_loss: 39455.0643\n",
      "Epoch 28/1000\n",
      "1168/1168 [==============================] - 0s 146us/step - loss: 33565.9343 - val_loss: 38888.9111\n",
      "Epoch 29/1000\n",
      "1168/1168 [==============================] - 0s 145us/step - loss: 33100.9240 - val_loss: 38368.0635\n",
      "Epoch 30/1000\n",
      "1168/1168 [==============================] - 0s 159us/step - loss: 32669.3631 - val_loss: 38049.3698\n",
      "Epoch 31/1000\n",
      "1168/1168 [==============================] - 0s 164us/step - loss: 32227.8059 - val_loss: 37526.1365\n",
      "Epoch 32/1000\n",
      "1168/1168 [==============================] - 0s 164us/step - loss: 32074.8958 - val_loss: 37143.4852\n",
      "Epoch 33/1000\n",
      "1168/1168 [==============================] - 0s 187us/step - loss: 31356.3984 - val_loss: 36755.3039\n",
      "Epoch 34/1000\n",
      "1168/1168 [==============================] - 0s 147us/step - loss: 30849.6017 - val_loss: 36416.5688\n",
      "Epoch 35/1000\n",
      "1168/1168 [==============================] - 0s 170us/step - loss: 30745.4138 - val_loss: 36261.0650\n",
      "Epoch 36/1000\n",
      "1168/1168 [==============================] - 0s 192us/step - loss: 30482.7650 - val_loss: 35788.0584\n",
      "Epoch 37/1000\n",
      "1168/1168 [==============================] - 0s 139us/step - loss: 29626.0815 - val_loss: 35498.0760\n",
      "Epoch 38/1000\n",
      "1168/1168 [==============================] - 0s 144us/step - loss: 29178.4564 - val_loss: 35233.8486\n",
      "Epoch 39/1000\n",
      "1168/1168 [==============================] - 0s 141us/step - loss: 29224.1289 - val_loss: 34994.2565\n",
      "Epoch 40/1000\n",
      "1168/1168 [==============================] - 0s 129us/step - loss: 28956.3932 - val_loss: 34800.4752\n",
      "Epoch 41/1000\n",
      "1168/1168 [==============================] - 0s 127us/step - loss: 28787.3741 - val_loss: 34560.0928\n",
      "Epoch 42/1000\n",
      "1168/1168 [==============================] - 0s 144us/step - loss: 28181.8337 - val_loss: 34410.3267\n",
      "Epoch 43/1000\n",
      "1168/1168 [==============================] - 0s 137us/step - loss: 28106.2374 - val_loss: 34273.9614\n",
      "Epoch 44/1000\n",
      "1168/1168 [==============================] - 0s 133us/step - loss: 28255.3398 - val_loss: 33998.3270\n",
      "Epoch 45/1000\n",
      "1168/1168 [==============================] - 0s 141us/step - loss: 27861.3051 - val_loss: 33913.7387\n",
      "Epoch 46/1000\n",
      "1168/1168 [==============================] - 0s 123us/step - loss: 27507.1566 - val_loss: 33687.7562\n",
      "Epoch 47/1000\n",
      "1168/1168 [==============================] - 0s 107us/step - loss: 27148.0272 - val_loss: 33549.3391\n",
      "Epoch 48/1000\n",
      "1168/1168 [==============================] - 0s 182us/step - loss: 27272.6186 - val_loss: 33431.5681\n",
      "Epoch 49/1000\n",
      "1168/1168 [==============================] - 0s 124us/step - loss: 27097.2297 - val_loss: 33289.1578\n",
      "Epoch 50/1000\n",
      "1168/1168 [==============================] - 0s 106us/step - loss: 26920.0075 - val_loss: 33201.2655\n",
      "Epoch 51/1000\n",
      "1168/1168 [==============================] - 0s 121us/step - loss: 26839.0543 - val_loss: 33173.0895\n",
      "Epoch 52/1000\n",
      "1168/1168 [==============================] - 0s 125us/step - loss: 26118.9283 - val_loss: 33002.2064\n",
      "Epoch 53/1000\n",
      "1168/1168 [==============================] - 0s 113us/step - loss: 26585.0780 - val_loss: 32873.3944\n",
      "Epoch 54/1000\n",
      "1168/1168 [==============================] - 0s 116us/step - loss: 26076.7130 - val_loss: 32739.2281\n",
      "Epoch 55/1000\n",
      "1168/1168 [==============================] - 0s 167us/step - loss: 26186.1577 - val_loss: 32735.7407\n",
      "Epoch 56/1000\n",
      "1168/1168 [==============================] - 0s 115us/step - loss: 25779.9112 - val_loss: 32566.6217\n",
      "Epoch 57/1000\n",
      "1168/1168 [==============================] - 0s 145us/step - loss: 25898.6612 - val_loss: 32478.2871\n",
      "Epoch 58/1000\n",
      "1168/1168 [==============================] - 0s 133us/step - loss: 25640.1279 - val_loss: 32380.2020\n",
      "Epoch 59/1000\n",
      "1168/1168 [==============================] - 0s 122us/step - loss: 25634.9097 - val_loss: 32336.3332\n",
      "Epoch 60/1000\n",
      "1168/1168 [==============================] - 0s 122us/step - loss: 25915.0098 - val_loss: 32190.6266\n",
      "Epoch 61/1000\n",
      "1168/1168 [==============================] - 0s 172us/step - loss: 25438.6034 - val_loss: 32121.1349\n",
      "Epoch 62/1000\n",
      "1168/1168 [==============================] - 0s 112us/step - loss: 25465.7029 - val_loss: 32084.7735\n",
      "Epoch 63/1000\n",
      "1168/1168 [==============================] - 0s 130us/step - loss: 25240.2003 - val_loss: 32054.6453\n",
      "Epoch 64/1000\n",
      "1168/1168 [==============================] - 0s 105us/step - loss: 24779.7159 - val_loss: 31893.2695\n",
      "Epoch 65/1000\n",
      "1168/1168 [==============================] - 0s 111us/step - loss: 24975.7410 - val_loss: 31833.7500\n",
      "Epoch 66/1000\n",
      "1168/1168 [==============================] - 0s 132us/step - loss: 24626.6321 - val_loss: 31752.9268\n",
      "Epoch 67/1000\n",
      "1168/1168 [==============================] - 0s 101us/step - loss: 24576.9032 - val_loss: 31691.4589\n",
      "Epoch 68/1000\n",
      "1168/1168 [==============================] - 0s 147us/step - loss: 24593.6403 - val_loss: 31756.6678\n",
      "Epoch 69/1000\n",
      "1168/1168 [==============================] - 0s 118us/step - loss: 24350.2263 - val_loss: 31588.9596\n",
      "Epoch 70/1000\n",
      "1168/1168 [==============================] - 0s 134us/step - loss: 24202.3418 - val_loss: 31625.7153\n",
      "Epoch 71/1000\n",
      "1168/1168 [==============================] - 0s 106us/step - loss: 24635.1233 - val_loss: 31725.7971\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 72/1000\n",
      "1168/1168 [==============================] - 0s 119us/step - loss: 24590.6074 - val_loss: 31385.6624\n",
      "Epoch 73/1000\n",
      "1168/1168 [==============================] - 0s 116us/step - loss: 24068.8681 - val_loss: 31308.0178\n",
      "Epoch 74/1000\n",
      "1168/1168 [==============================] - 0s 107us/step - loss: 24261.5335 - val_loss: 31344.3576\n",
      "Epoch 75/1000\n",
      "1168/1168 [==============================] - 0s 180us/step - loss: 24103.6849 - val_loss: 31241.7791\n",
      "Epoch 76/1000\n",
      "1168/1168 [==============================] - 0s 160us/step - loss: 24047.0442 - val_loss: 31169.4680\n",
      "Epoch 77/1000\n",
      "1168/1168 [==============================] - 0s 142us/step - loss: 23658.0887 - val_loss: 31130.4729\n",
      "Epoch 78/1000\n",
      "1168/1168 [==============================] - 0s 117us/step - loss: 23644.1785 - val_loss: 31105.9626\n",
      "Epoch 79/1000\n",
      "1168/1168 [==============================] - 0s 117us/step - loss: 23565.8600 - val_loss: 31162.1307\n",
      "Epoch 80/1000\n",
      "1168/1168 [==============================] - 0s 116us/step - loss: 23832.3672 - val_loss: 31014.1860\n",
      "Epoch 81/1000\n",
      "1168/1168 [==============================] - 0s 134us/step - loss: 23830.9429 - val_loss: 30990.1863\n",
      "Epoch 82/1000\n",
      "1168/1168 [==============================] - 0s 104us/step - loss: 23169.2265 - val_loss: 30944.0313\n",
      "Epoch 83/1000\n",
      "1168/1168 [==============================] - 0s 104us/step - loss: 23333.6647 - val_loss: 30906.3252\n",
      "Epoch 84/1000\n",
      "1168/1168 [==============================] - 0s 111us/step - loss: 23120.7879 - val_loss: 30937.4000\n",
      "Epoch 85/1000\n",
      "1168/1168 [==============================] - 0s 114us/step - loss: 23051.6441 - val_loss: 30828.3329\n",
      "Epoch 86/1000\n",
      "1168/1168 [==============================] - 0s 113us/step - loss: 23298.9727 - val_loss: 30778.6952\n",
      "Epoch 87/1000\n",
      "1168/1168 [==============================] - 0s 107us/step - loss: 23143.8160 - val_loss: 30748.8677\n",
      "Epoch 88/1000\n",
      "1168/1168 [==============================] - 0s 115us/step - loss: 23183.4839 - val_loss: 30723.0231\n",
      "Epoch 89/1000\n",
      "1168/1168 [==============================] - 0s 177us/step - loss: 22793.2090 - val_loss: 30842.2133\n",
      "Epoch 90/1000\n",
      "1168/1168 [==============================] - 0s 117us/step - loss: 23067.8059 - val_loss: 30921.6952\n",
      "Epoch 91/1000\n",
      "1168/1168 [==============================] - 0s 123us/step - loss: 22786.0577 - val_loss: 30662.2368\n",
      "Epoch 92/1000\n",
      "1168/1168 [==============================] - 0s 105us/step - loss: 22930.0020 - val_loss: 30626.0164\n",
      "Epoch 93/1000\n",
      "1168/1168 [==============================] - 0s 111us/step - loss: 22800.2018 - val_loss: 30753.1232\n",
      "Epoch 94/1000\n",
      "1168/1168 [==============================] - 0s 114us/step - loss: 22908.8313 - val_loss: 30580.3705\n",
      "Epoch 95/1000\n",
      "1168/1168 [==============================] - 0s 114us/step - loss: 22854.7161 - val_loss: 30624.4853\n",
      "Epoch 96/1000\n",
      "1168/1168 [==============================] - 0s 152us/step - loss: 22690.4367 - val_loss: 30631.2073\n",
      "Epoch 97/1000\n",
      "1168/1168 [==============================] - 0s 122us/step - loss: 22585.6523 - val_loss: 30556.9332\n",
      "Epoch 98/1000\n",
      "1168/1168 [==============================] - 0s 122us/step - loss: 22730.9160 - val_loss: 30542.2297\n",
      "Epoch 99/1000\n",
      "1168/1168 [==============================] - 0s 111us/step - loss: 22567.5319 - val_loss: 30513.5401\n",
      "Epoch 100/1000\n",
      "1168/1168 [==============================] - 0s 127us/step - loss: 22357.5745 - val_loss: 30670.5990\n",
      "Epoch 101/1000\n",
      "1168/1168 [==============================] - 0s 101us/step - loss: 22134.9942 - val_loss: 30503.1643\n",
      "Epoch 102/1000\n",
      "1168/1168 [==============================] - 0s 103us/step - loss: 22299.8547 - val_loss: 30460.1368\n",
      "Epoch 103/1000\n",
      "1168/1168 [==============================] - 0s 171us/step - loss: 22128.9090 - val_loss: 30458.2086\n",
      "Epoch 104/1000\n",
      "1168/1168 [==============================] - 0s 115us/step - loss: 22121.2491 - val_loss: 30458.3981\n",
      "Epoch 105/1000\n",
      "1168/1168 [==============================] - 0s 115us/step - loss: 22191.2713 - val_loss: 30431.0910\n",
      "Epoch 106/1000\n",
      "1168/1168 [==============================] - 0s 106us/step - loss: 22273.1414 - val_loss: 30365.3134\n",
      "Epoch 107/1000\n",
      "1168/1168 [==============================] - 0s 128us/step - loss: 22106.2865 - val_loss: 30364.2894\n",
      "Epoch 108/1000\n",
      "1168/1168 [==============================] - 0s 104us/step - loss: 21851.1439 - val_loss: 30371.8884\n",
      "Epoch 109/1000\n",
      "1168/1168 [==============================] - 0s 113us/step - loss: 21916.3554 - val_loss: 30324.2424\n",
      "Epoch 110/1000\n",
      "1168/1168 [==============================] - 0s 133us/step - loss: 22106.7621 - val_loss: 30337.6882\n",
      "Epoch 111/1000\n",
      "1168/1168 [==============================] - 0s 119us/step - loss: 21881.9188 - val_loss: 30249.2905\n",
      "Epoch 112/1000\n",
      "1168/1168 [==============================] - 0s 99us/step - loss: 21775.4961 - val_loss: 30257.8687\n",
      "Epoch 113/1000\n",
      "1168/1168 [==============================] - 0s 111us/step - loss: 21836.3965 - val_loss: 30443.4174\n",
      "Epoch 114/1000\n",
      "1168/1168 [==============================] - 0s 116us/step - loss: 21902.7559 - val_loss: 30209.1106\n",
      "Epoch 115/1000\n",
      "1168/1168 [==============================] - 0s 132us/step - loss: 21841.2684 - val_loss: 30171.2594\n",
      "Epoch 116/1000\n",
      "1168/1168 [==============================] - 0s 104us/step - loss: 21653.4163 - val_loss: 30179.0151\n",
      "Epoch 117/1000\n",
      "1168/1168 [==============================] - 0s 184us/step - loss: 21889.6087 - val_loss: 30244.9876\n",
      "Epoch 118/1000\n",
      "1168/1168 [==============================] - 0s 112us/step - loss: 21533.4516 - val_loss: 30161.0124\n",
      "Epoch 119/1000\n",
      "1168/1168 [==============================] - 0s 123us/step - loss: 21549.4655 - val_loss: 30136.3590\n",
      "Epoch 120/1000\n",
      "1168/1168 [==============================] - 0s 120us/step - loss: 21534.3546 - val_loss: 30294.4752\n",
      "Epoch 121/1000\n",
      "1168/1168 [==============================] - 0s 102us/step - loss: 21682.0568 - val_loss: 30115.1661\n",
      "Epoch 122/1000\n",
      "1168/1168 [==============================] - 0s 115us/step - loss: 21566.8757 - val_loss: 30097.6099\n",
      "Epoch 123/1000\n",
      "1168/1168 [==============================] - 0s 104us/step - loss: 21536.5384 - val_loss: 30183.4959\n",
      "Epoch 124/1000\n",
      "1168/1168 [==============================] - 0s 146us/step - loss: 21334.9577 - val_loss: 30124.9821\n",
      "Epoch 125/1000\n",
      "1168/1168 [==============================] - 0s 111us/step - loss: 21295.5411 - val_loss: 30220.6760\n",
      "Epoch 126/1000\n",
      "1168/1168 [==============================] - 0s 135us/step - loss: 21024.9978 - val_loss: 30165.0913\n",
      "Epoch 127/1000\n",
      "1168/1168 [==============================] - 0s 117us/step - loss: 21300.2423 - val_loss: 30096.1446\n",
      "Epoch 128/1000\n",
      "1168/1168 [==============================] - 0s 134us/step - loss: 21074.3062 - val_loss: 30163.6236\n",
      "Epoch 129/1000\n",
      "1168/1168 [==============================] - 0s 102us/step - loss: 21332.3333 - val_loss: 30255.9207\n",
      "Epoch 130/1000\n",
      "1168/1168 [==============================] - 0s 113us/step - loss: 21241.0543 - val_loss: 30076.8978\n",
      "Epoch 131/1000\n",
      "1168/1168 [==============================] - 0s 154us/step - loss: 20887.1614 - val_loss: 30118.4482\n",
      "Epoch 132/1000\n",
      "1168/1168 [==============================] - 0s 121us/step - loss: 21255.9621 - val_loss: 30007.7950\n",
      "Epoch 133/1000\n",
      "1168/1168 [==============================] - 0s 114us/step - loss: 21156.1920 - val_loss: 30015.1161\n",
      "Epoch 134/1000\n",
      "1168/1168 [==============================] - 0s 121us/step - loss: 20597.4967 - val_loss: 30153.6925\n",
      "Epoch 135/1000\n",
      "1168/1168 [==============================] - 0s 111us/step - loss: 20941.8434 - val_loss: 29968.6862\n",
      "Epoch 136/1000\n",
      "1168/1168 [==============================] - 0s 111us/step - loss: 21050.2496 - val_loss: 29993.2844\n",
      "Epoch 137/1000\n",
      "1168/1168 [==============================] - 0s 112us/step - loss: 20963.1975 - val_loss: 29999.6515\n",
      "Epoch 138/1000\n",
      "1168/1168 [==============================] - 0s 179us/step - loss: 20807.5189 - val_loss: 29995.1926\n",
      "Epoch 139/1000\n",
      "1168/1168 [==============================] - 0s 135us/step - loss: 21102.8037 - val_loss: 30106.2526\n",
      "Epoch 140/1000\n",
      "1168/1168 [==============================] - 0s 113us/step - loss: 20676.0944 - val_loss: 30083.3239\n",
      "Epoch 141/1000\n",
      "1168/1168 [==============================] - 0s 120us/step - loss: 20798.1765 - val_loss: 30027.6619\n",
      "Epoch 142/1000\n",
      "1168/1168 [==============================] - 0s 111us/step - loss: 20831.4253 - val_loss: 30004.6636\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 143/1000\n",
      "1168/1168 [==============================] - 0s 122us/step - loss: 20661.7063 - val_loss: 29946.8882\n",
      "Epoch 144/1000\n",
      "1168/1168 [==============================] - 0s 139us/step - loss: 20802.4150 - val_loss: 30005.7758\n",
      "Epoch 145/1000\n",
      "1168/1168 [==============================] - 0s 145us/step - loss: 20678.4350 - val_loss: 30034.8205\n",
      "Epoch 146/1000\n",
      "1168/1168 [==============================] - 0s 111us/step - loss: 20520.2458 - val_loss: 29960.4418\n",
      "Epoch 147/1000\n",
      "1168/1168 [==============================] - 0s 111us/step - loss: 20583.3970 - val_loss: 29925.9410\n",
      "Epoch 148/1000\n",
      "1168/1168 [==============================] - 0s 128us/step - loss: 20515.2165 - val_loss: 29948.0088\n",
      "Epoch 149/1000\n",
      "1168/1168 [==============================] - 0s 108us/step - loss: 20684.4015 - val_loss: 29953.4346\n",
      "Epoch 150/1000\n",
      "1168/1168 [==============================] - 0s 134us/step - loss: 20339.1180 - val_loss: 30101.4613\n",
      "Epoch 151/1000\n",
      "1168/1168 [==============================] - 0s 126us/step - loss: 20230.0796 - val_loss: 29910.3537\n",
      "Epoch 152/1000\n",
      "1168/1168 [==============================] - 0s 108us/step - loss: 20637.4295 - val_loss: 29967.8180\n",
      "Epoch 153/1000\n",
      "1168/1168 [==============================] - 0s 114us/step - loss: 20514.7079 - val_loss: 30015.8669\n",
      "Epoch 154/1000\n",
      "1168/1168 [==============================] - 0s 124us/step - loss: 20399.3421 - val_loss: 30345.1182\n",
      "Epoch 155/1000\n",
      "1168/1168 [==============================] - 0s 117us/step - loss: 20517.7383 - val_loss: 29950.1480\n",
      "Epoch 156/1000\n",
      "1168/1168 [==============================] - 0s 112us/step - loss: 20332.2409 - val_loss: 30014.8651\n",
      "Epoch 157/1000\n",
      "1168/1168 [==============================] - 0s 113us/step - loss: 20296.7258 - val_loss: 29939.1300\n",
      "Epoch 158/1000\n",
      "1168/1168 [==============================] - 0s 156us/step - loss: 20236.2908 - val_loss: 30007.8937\n",
      "Epoch 159/1000\n",
      "1168/1168 [==============================] - 0s 141us/step - loss: 20421.1332 - val_loss: 29921.3694\n",
      "Epoch 160/1000\n",
      "1168/1168 [==============================] - 0s 111us/step - loss: 20355.4132 - val_loss: 30019.0090\n",
      "Epoch 161/1000\n",
      "1168/1168 [==============================] - 0s 107us/step - loss: 20425.5965 - val_loss: 29910.0605\n",
      "Epoch 162/1000\n",
      "1168/1168 [==============================] - 0s 115us/step - loss: 20338.6761 - val_loss: 30031.3521\n",
      "Epoch 163/1000\n",
      "1168/1168 [==============================] - 0s 117us/step - loss: 20207.4584 - val_loss: 29943.8983\n",
      "Epoch 164/1000\n",
      "1168/1168 [==============================] - 0s 114us/step - loss: 20192.9578 - val_loss: 30103.7132\n",
      "Epoch 165/1000\n",
      "1168/1168 [==============================] - 0s 140us/step - loss: 20315.3146 - val_loss: 30117.2195\n",
      "Epoch 166/1000\n",
      "1168/1168 [==============================] - 0s 110us/step - loss: 20080.5600 - val_loss: 30158.1660\n",
      "Epoch 167/1000\n",
      "1168/1168 [==============================] - 0s 130us/step - loss: 20286.7681 - val_loss: 29938.4752\n",
      "Epoch 168/1000\n",
      "1168/1168 [==============================] - 0s 100us/step - loss: 19859.8237 - val_loss: 29976.6497\n",
      "Epoch 169/1000\n",
      "1168/1168 [==============================] - 0s 114us/step - loss: 20226.2302 - val_loss: 29946.4483\n",
      "Epoch 170/1000\n",
      "1168/1168 [==============================] - 0s 117us/step - loss: 19927.9851 - val_loss: 29987.8775\n",
      "Epoch 171/1000\n",
      "1168/1168 [==============================] - 0s 117us/step - loss: 19897.0965 - val_loss: 29950.9368\n",
      "Epoch 172/1000\n",
      "1168/1168 [==============================] - 0s 141us/step - loss: 20034.8233 - val_loss: 30186.0875\n",
      "Epoch 173/1000\n",
      "1168/1168 [==============================] - 0s 127us/step - loss: 19878.3460 - val_loss: 29957.6868\n",
      "Epoch 174/1000\n",
      "1168/1168 [==============================] - ETA: 0s - loss: 19802.360 - 0s 111us/step - loss: 19983.5709 - val_loss: 29939.5057\n",
      "Epoch 175/1000\n",
      "1168/1168 [==============================] - 0s 130us/step - loss: 19943.8881 - val_loss: 30099.0991\n",
      "Epoch 176/1000\n",
      "1168/1168 [==============================] - 0s 106us/step - loss: 20017.4052 - val_loss: 30069.5566\n",
      "Epoch 177/1000\n",
      "1168/1168 [==============================] - 0s 107us/step - loss: 19841.7393 - val_loss: 30025.5154\n",
      "Epoch 178/1000\n",
      "1168/1168 [==============================] - 0s 117us/step - loss: 19605.8594 - val_loss: 29931.5464\n",
      "Epoch 179/1000\n",
      "1168/1168 [==============================] - 0s 171us/step - loss: 19671.1534 - val_loss: 30086.5915\n",
      "Epoch 180/1000\n",
      "1168/1168 [==============================] - 0s 117us/step - loss: 19728.2980 - val_loss: 29946.7784\n",
      "Epoch 181/1000\n",
      "1168/1168 [==============================] - 0s 106us/step - loss: 19896.6834 - val_loss: 30014.2406\n",
      "Epoch 182/1000\n",
      "1168/1168 [==============================] - 0s 124us/step - loss: 19772.7798 - val_loss: 30061.2765\n",
      "Epoch 183/1000\n",
      "1168/1168 [==============================] - 0s 106us/step - loss: 19524.5621 - val_loss: 30019.6486\n",
      "Epoch 184/1000\n",
      "1168/1168 [==============================] - 0s 114us/step - loss: 19576.7561 - val_loss: 29921.9867\n",
      "Epoch 185/1000\n",
      "1168/1168 [==============================] - 0s 118us/step - loss: 19844.6696 - val_loss: 30080.5934\n",
      "Epoch 186/1000\n",
      "1168/1168 [==============================] - ETA: 0s - loss: 19793.812 - 0s 161us/step - loss: 19549.1767 - val_loss: 29943.1097\n",
      "Epoch 187/1000\n",
      "1168/1168 [==============================] - 0s 129us/step - loss: 19821.1175 - val_loss: 30164.1839\n",
      "Epoch 188/1000\n",
      "1168/1168 [==============================] - 0s 111us/step - loss: 19490.9658 - val_loss: 29907.6113\n",
      "Epoch 189/1000\n",
      "1168/1168 [==============================] - 0s 134us/step - loss: 19477.4088 - val_loss: 29900.0726\n",
      "Epoch 190/1000\n",
      "1168/1168 [==============================] - 0s 105us/step - loss: 19518.3658 - val_loss: 29848.6900\n",
      "Epoch 191/1000\n",
      "1168/1168 [==============================] - 0s 113us/step - loss: 19575.1879 - val_loss: 29895.4542\n",
      "Epoch 192/1000\n",
      "1168/1168 [==============================] - 0s 138us/step - loss: 19330.7688 - val_loss: 30206.9376\n",
      "Epoch 193/1000\n",
      "1168/1168 [==============================] - 0s 137us/step - loss: 19523.6916 - val_loss: 29914.8090\n",
      "Epoch 194/1000\n",
      "1168/1168 [==============================] - 0s 102us/step - loss: 19177.5645 - val_loss: 29898.3916\n",
      "Epoch 195/1000\n",
      "1168/1168 [==============================] - 0s 118us/step - loss: 19254.0427 - val_loss: 29906.7436\n",
      "Epoch 196/1000\n",
      "1168/1168 [==============================] - 0s 111us/step - loss: 19267.2295 - val_loss: 29894.5478\n",
      "Epoch 197/1000\n",
      "1168/1168 [==============================] - 0s 125us/step - loss: 19253.6805 - val_loss: 29911.8798\n",
      "Epoch 198/1000\n",
      "1168/1168 [==============================] - 0s 112us/step - loss: 19221.5650 - val_loss: 29927.7670\n",
      "Epoch 199/1000\n",
      "1168/1168 [==============================] - 0s 115us/step - loss: 19315.3991 - val_loss: 29924.2470\n",
      "Epoch 200/1000\n",
      "1168/1168 [==============================] - 0s 170us/step - loss: 19296.5811 - val_loss: 29919.0503\n",
      "Epoch 201/1000\n",
      "1168/1168 [==============================] - 0s 118us/step - loss: 19196.6498 - val_loss: 29986.4391\n",
      "Epoch 202/1000\n",
      "1168/1168 [==============================] - 0s 120us/step - loss: 19506.5354 - val_loss: 29941.4697\n",
      "Epoch 203/1000\n",
      "1168/1168 [==============================] - 0s 119us/step - loss: 19142.5940 - val_loss: 30051.2516\n",
      "Epoch 204/1000\n",
      "1168/1168 [==============================] - 0s 108us/step - loss: 19463.1834 - val_loss: 30006.6437\n",
      "Epoch 205/1000\n",
      "1168/1168 [==============================] - 0s 120us/step - loss: 19246.3180 - val_loss: 29956.6088\n",
      "Epoch 206/1000\n",
      "1168/1168 [==============================] - 0s 117us/step - loss: 18980.3150 - val_loss: 29948.2979\n",
      "Epoch 207/1000\n",
      "1168/1168 [==============================] - 0s 139us/step - loss: 19135.2047 - val_loss: 29953.9566\n",
      "Epoch 208/1000\n",
      "1168/1168 [==============================] - 0s 105us/step - loss: 19316.2932 - val_loss: 29884.7369\n",
      "Epoch 209/1000\n",
      "1168/1168 [==============================] - 0s 120us/step - loss: 19147.7099 - val_loss: 29878.4915\n",
      "Epoch 210/1000\n",
      "1168/1168 [==============================] - 0s 116us/step - loss: 19267.1861 - val_loss: 29869.2225\n",
      "Epoch 211/1000\n",
      "1168/1168 [==============================] - 0s 110us/step - loss: 19115.5523 - val_loss: 29990.9029\n",
      "Epoch 212/1000\n",
      "1168/1168 [==============================] - 0s 138us/step - loss: 19144.8674 - val_loss: 29986.6250\n",
      "Epoch 213/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1168/1168 [==============================] - 0s 121us/step - loss: 19033.2894 - val_loss: 29992.8809\n",
      "Epoch 214/1000\n",
      "1168/1168 [==============================] - 0s 150us/step - loss: 19227.9172 - val_loss: 30009.6653\n",
      "Epoch 215/1000\n",
      "1168/1168 [==============================] - 0s 110us/step - loss: 18838.1869 - val_loss: 30131.7468\n",
      "Epoch 216/1000\n",
      "1168/1168 [==============================] - 0s 117us/step - loss: 19156.8908 - val_loss: 29923.3623\n",
      "Epoch 217/1000\n",
      "1168/1168 [==============================] - 0s 113us/step - loss: 18686.8071 - val_loss: 29931.4356\n",
      "Epoch 218/1000\n",
      "1168/1168 [==============================] - 0s 123us/step - loss: 19032.1636 - val_loss: 29903.6066\n",
      "Epoch 219/1000\n",
      "1168/1168 [==============================] - 0s 106us/step - loss: 18971.9436 - val_loss: 29995.7542\n",
      "Epoch 220/1000\n",
      "1168/1168 [==============================] - 0s 114us/step - loss: 18972.9945 - val_loss: 30069.5872\n",
      "Epoch 221/1000\n",
      "1168/1168 [==============================] - 0s 169us/step - loss: 18946.1975 - val_loss: 30185.6249\n",
      "Epoch 222/1000\n",
      "1168/1168 [==============================] - 0s 103us/step - loss: 19369.3931 - val_loss: 29970.4026\n",
      "Epoch 223/1000\n",
      "1168/1168 [==============================] - 0s 112us/step - loss: 18892.7169 - val_loss: 30044.8464\n",
      "Epoch 224/1000\n",
      "1168/1168 [==============================] - 0s 117us/step - loss: 18786.5486 - val_loss: 30030.5292\n",
      "Epoch 225/1000\n",
      "1168/1168 [==============================] - 0s 117us/step - loss: 19064.6175 - val_loss: 29951.7320\n",
      "Epoch 226/1000\n",
      "1168/1168 [==============================] - 0s 111us/step - loss: 18877.5723 - val_loss: 30172.0945\n",
      "Epoch 227/1000\n",
      "1168/1168 [==============================] - 0s 115us/step - loss: 18785.7275 - val_loss: 30113.1896\n",
      "Epoch 228/1000\n",
      "1168/1168 [==============================] - 0s 177us/step - loss: 18700.1153 - val_loss: 29941.4186\n",
      "Epoch 229/1000\n",
      "1168/1168 [==============================] - 0s 116us/step - loss: 18816.5329 - val_loss: 29930.4866\n",
      "Epoch 230/1000\n",
      "1168/1168 [==============================] - 0s 117us/step - loss: 18610.8919 - val_loss: 30313.4165\n",
      "Epoch 231/1000\n",
      "1168/1168 [==============================] - 0s 106us/step - loss: 18956.4223 - val_loss: 29961.5883\n",
      "Epoch 232/1000\n",
      "1168/1168 [==============================] - 0s 110us/step - loss: 18730.2367 - val_loss: 29988.9240\n",
      "Epoch 233/1000\n",
      "1168/1168 [==============================] - 0s 112us/step - loss: 18796.8707 - val_loss: 30163.9531\n",
      "Epoch 234/1000\n",
      "1168/1168 [==============================] - 0s 117us/step - loss: 18811.4826 - val_loss: 30046.2718\n",
      "Epoch 235/1000\n",
      "1168/1168 [==============================] - 0s 135us/step - loss: 18667.0956 - val_loss: 29909.6519\n",
      "Epoch 236/1000\n",
      "1168/1168 [==============================] - 0s 117us/step - loss: 18597.3974 - val_loss: 29984.8511\n",
      "Epoch 237/1000\n",
      "1168/1168 [==============================] - 0s 126us/step - loss: 18699.7994 - val_loss: 29921.3911\n",
      "Epoch 238/1000\n",
      "1168/1168 [==============================] - 0s 104us/step - loss: 18747.1755 - val_loss: 29913.5825\n",
      "Epoch 239/1000\n",
      "1168/1168 [==============================] - 0s 114us/step - loss: 18805.7705 - val_loss: 29934.4010\n",
      "Epoch 240/1000\n",
      "1168/1168 [==============================] - 0s 113us/step - loss: 18567.1665 - val_loss: 30172.8766\n",
      "Epoch 241/1000\n",
      "1168/1168 [==============================] - 0s 112us/step - loss: 18654.4390 - val_loss: 29932.3364\n",
      "Epoch 242/1000\n",
      "1168/1168 [==============================] - 0s 171us/step - loss: 18593.8779 - val_loss: 29964.1361\n",
      "Epoch 243/1000\n",
      "1168/1168 [==============================] - 0s 120us/step - loss: 18531.1088 - val_loss: 30082.7644\n",
      "Epoch 244/1000\n",
      "1168/1168 [==============================] - 0s 128us/step - loss: 18851.7479 - val_loss: 29961.6097\n",
      "Epoch 245/1000\n",
      "1168/1168 [==============================] - 0s 106us/step - loss: 18419.9198 - val_loss: 30029.6956\n",
      "Epoch 246/1000\n",
      "1168/1168 [==============================] - 0s 114us/step - loss: 18485.0785 - val_loss: 29973.0888\n",
      "Epoch 247/1000\n",
      "1168/1168 [==============================] - 0s 123us/step - loss: 18618.2353 - val_loss: 30320.5258\n",
      "Epoch 248/1000\n",
      "1168/1168 [==============================] - 0s 104us/step - loss: 18653.3309 - val_loss: 30216.6169\n",
      "Epoch 249/1000\n",
      "1168/1168 [==============================] - 0s 148us/step - loss: 18400.1833 - val_loss: 29983.3401\n",
      "Epoch 250/1000\n",
      "1168/1168 [==============================] - 0s 117us/step - loss: 18113.1450 - val_loss: 30020.5919\n",
      "Epoch 251/1000\n",
      "1168/1168 [==============================] - 0s 136us/step - loss: 18412.3893 - val_loss: 29919.7580\n",
      "Epoch 252/1000\n",
      "1168/1168 [==============================] - 0s 117us/step - loss: 18462.9038 - val_loss: 29969.0739\n",
      "Epoch 253/1000\n",
      "1168/1168 [==============================] - 0s 121us/step - loss: 18381.4324 - val_loss: 29982.4327\n",
      "Epoch 254/1000\n",
      "1168/1168 [==============================] - 0s 109us/step - loss: 18378.0827 - val_loss: 29962.4020\n",
      "Epoch 255/1000\n",
      "1168/1168 [==============================] - 0s 110us/step - loss: 18712.6811 - val_loss: 30065.5883\n",
      "Epoch 256/1000\n",
      "1168/1168 [==============================] - 0s 161us/step - loss: 18515.2536 - val_loss: 29968.0594\n",
      "Epoch 257/1000\n",
      "1168/1168 [==============================] - 0s 117us/step - loss: 18483.6405 - val_loss: 29943.1295\n",
      "Epoch 258/1000\n",
      "1168/1168 [==============================] - 0s 108us/step - loss: 18221.4965 - val_loss: 30033.8591\n",
      "Epoch 259/1000\n",
      "1168/1168 [==============================] - 0s 119us/step - loss: 18334.0248 - val_loss: 30162.7141\n",
      "Epoch 260/1000\n",
      "1168/1168 [==============================] - 0s 135us/step - loss: 18276.9370 - val_loss: 29913.0276\n",
      "Epoch 261/1000\n",
      "1168/1168 [==============================] - 0s 117us/step - loss: 18277.5435 - val_loss: 29946.5569\n",
      "Epoch 262/1000\n",
      "1168/1168 [==============================] - 0s 115us/step - loss: 18340.0907 - val_loss: 29931.1553\n",
      "Epoch 263/1000\n",
      "1168/1168 [==============================] - 0s 161us/step - loss: 18533.6767 - val_loss: 29943.6787\n",
      "Epoch 264/1000\n",
      "1168/1168 [==============================] - 0s 122us/step - loss: 18228.8867 - val_loss: 30016.7274\n",
      "Epoch 265/1000\n",
      "1168/1168 [==============================] - 0s 111us/step - loss: 18324.7104 - val_loss: 29974.4522\n",
      "Epoch 266/1000\n",
      "1168/1168 [==============================] - 0s 105us/step - loss: 18479.0368 - val_loss: 29993.1254\n",
      "Epoch 267/1000\n",
      "1168/1168 [==============================] - 0s 111us/step - loss: 18203.5915 - val_loss: 29902.1359\n",
      "Epoch 268/1000\n",
      "1168/1168 [==============================] - 0s 115us/step - loss: 18248.6776 - val_loss: 29958.1130\n",
      "Epoch 269/1000\n",
      "1168/1168 [==============================] - 0s 123us/step - loss: 18198.0663 - val_loss: 30043.2432\n",
      "Epoch 270/1000\n",
      "1168/1168 [==============================] - 0s 147us/step - loss: 18373.9225 - val_loss: 29925.6720\n",
      "Epoch 271/1000\n",
      "1168/1168 [==============================] - 0s 115us/step - loss: 18152.6491 - val_loss: 29963.1613\n",
      "Epoch 272/1000\n",
      "1168/1168 [==============================] - 0s 128us/step - loss: 18339.4475 - val_loss: 30382.6977\n",
      "Epoch 273/1000\n",
      "1168/1168 [==============================] - 0s 163us/step - loss: 18110.8436 - val_loss: 30065.2872\n",
      "Epoch 274/1000\n",
      "1168/1168 [==============================] - 0s 107us/step - loss: 18214.6982 - val_loss: 29984.9937\n",
      "Epoch 275/1000\n",
      "1168/1168 [==============================] - 0s 105us/step - loss: 18141.5737 - val_loss: 29971.7638\n",
      "Epoch 276/1000\n",
      "1168/1168 [==============================] - 0s 147us/step - loss: 18129.5579 - val_loss: 29942.9522\n",
      "Epoch 277/1000\n",
      "1168/1168 [==============================] - 0s 211us/step - loss: 18058.3640 - val_loss: 30109.6665\n",
      "Epoch 278/1000\n",
      "1168/1168 [==============================] - 0s 208us/step - loss: 18172.2018 - val_loss: 30087.7602\n",
      "Epoch 279/1000\n",
      "1168/1168 [==============================] - 0s 196us/step - loss: 18267.5032 - val_loss: 29985.5434\n",
      "Epoch 280/1000\n",
      "1168/1168 [==============================] - 0s 201us/step - loss: 17970.9866 - val_loss: 30470.0470\n",
      "Epoch 281/1000\n",
      "1168/1168 [==============================] - 0s 251us/step - loss: 18094.6974 - val_loss: 29974.0128\n",
      "Epoch 282/1000\n",
      "1168/1168 [==============================] - 0s 196us/step - loss: 18085.5743 - val_loss: 30048.0655\n",
      "Epoch 283/1000\n",
      "1168/1168 [==============================] - 0s 244us/step - loss: 18096.0693 - val_loss: 30161.4519\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 284/1000\n",
      "1168/1168 [==============================] - 0s 248us/step - loss: 17998.3783 - val_loss: 30035.5992\n",
      "Epoch 285/1000\n",
      "1168/1168 [==============================] - 0s 216us/step - loss: 18093.2218 - val_loss: 29976.2424\n",
      "Epoch 286/1000\n",
      "1168/1168 [==============================] - 0s 190us/step - loss: 18125.3765 - val_loss: 29948.7310\n",
      "Epoch 287/1000\n",
      "1168/1168 [==============================] - 0s 220us/step - loss: 18023.2602 - val_loss: 29981.4833\n",
      "Epoch 288/1000\n",
      "1168/1168 [==============================] - 0s 246us/step - loss: 17911.6320 - val_loss: 30106.5749\n",
      "Epoch 289/1000\n",
      "1168/1168 [==============================] - 0s 170us/step - loss: 18163.0993 - val_loss: 29996.7199\n",
      "Epoch 290/1000\n",
      "1168/1168 [==============================] - 0s 236us/step - loss: 17803.8722 - val_loss: 30143.4154\n",
      "Epoch 291/1000\n",
      "1168/1168 [==============================] - 0s 217us/step - loss: 17979.1655 - val_loss: 30081.4826\n",
      "Epoch 292/1000\n",
      "1168/1168 [==============================] - 0s 228us/step - loss: 17911.6171 - val_loss: 30156.9784\n",
      "Epoch 293/1000\n",
      "1168/1168 [==============================] - 0s 198us/step - loss: 17785.1951 - val_loss: 30252.4876\n",
      "Epoch 294/1000\n",
      "1168/1168 [==============================] - 0s 194us/step - loss: 18172.4467 - val_loss: 30136.5485\n",
      "Epoch 295/1000\n",
      "1168/1168 [==============================] - 0s 184us/step - loss: 18033.7412 - val_loss: 30058.6564\n",
      "Epoch 296/1000\n",
      "1168/1168 [==============================] - 0s 204us/step - loss: 18078.1124 - val_loss: 30132.7671\n",
      "Epoch 297/1000\n",
      "1168/1168 [==============================] - 0s 174us/step - loss: 17812.0878 - val_loss: 30178.3593\n",
      "Epoch 298/1000\n",
      "1168/1168 [==============================] - 0s 112us/step - loss: 17712.9823 - val_loss: 30101.4027\n",
      "Epoch 299/1000\n",
      "1168/1168 [==============================] - 0s 133us/step - loss: 17867.8936 - val_loss: 30227.1500\n",
      "Epoch 300/1000\n",
      "1168/1168 [==============================] - 0s 121us/step - loss: 17916.3482 - val_loss: 30131.2908\n",
      "Epoch 301/1000\n",
      "1168/1168 [==============================] - 0s 120us/step - loss: 17934.0700 - val_loss: 30034.8952\n",
      "Epoch 302/1000\n",
      "1168/1168 [==============================] - 0s 174us/step - loss: 17829.0508 - val_loss: 30054.4056\n",
      "Epoch 303/1000\n",
      "1168/1168 [==============================] - 0s 108us/step - loss: 17616.8451 - val_loss: 30176.6098\n",
      "Epoch 304/1000\n",
      "1168/1168 [==============================] - 0s 123us/step - loss: 18036.8564 - val_loss: 30019.2897\n",
      "Epoch 305/1000\n",
      "1168/1168 [==============================] - 0s 125us/step - loss: 17678.2878 - val_loss: 30147.2578\n",
      "Epoch 306/1000\n",
      "1168/1168 [==============================] - 0s 130us/step - loss: 17786.8214 - val_loss: 29999.1839\n",
      "Epoch 307/1000\n",
      "1168/1168 [==============================] - 0s 118us/step - loss: 17782.5464 - val_loss: 30157.5831\n",
      "Epoch 308/1000\n",
      "1168/1168 [==============================] - 0s 127us/step - loss: 17547.2235 - val_loss: 30062.8893\n",
      "Epoch 309/1000\n",
      "1168/1168 [==============================] - 0s 192us/step - loss: 17657.8588 - val_loss: 30231.1455\n",
      "Epoch 310/1000\n",
      "1168/1168 [==============================] - 0s 124us/step - loss: 17845.4184 - val_loss: 30051.6102\n",
      "Epoch 311/1000\n",
      "1168/1168 [==============================] - 0s 122us/step - loss: 17759.2724 - val_loss: 30317.8670\n",
      "Epoch 312/1000\n",
      "1168/1168 [==============================] - 0s 128us/step - loss: 17662.3102 - val_loss: 30205.8533\n",
      "Epoch 313/1000\n",
      "1168/1168 [==============================] - 0s 122us/step - loss: 17770.4489 - val_loss: 30036.6253\n",
      "Epoch 314/1000\n",
      "1168/1168 [==============================] - 0s 129us/step - loss: 17461.3743 - val_loss: 30050.4085\n",
      "Epoch 315/1000\n",
      "1168/1168 [==============================] - 0s 138us/step - loss: 17700.1380 - val_loss: 30077.1740\n",
      "Epoch 316/1000\n",
      "1168/1168 [==============================] - 0s 126us/step - loss: 17768.5213 - val_loss: 30223.3988\n",
      "Epoch 317/1000\n",
      "1168/1168 [==============================] - 0s 115us/step - loss: 17492.8593 - val_loss: 30121.6114\n",
      "Epoch 318/1000\n",
      "1168/1168 [==============================] - 0s 133us/step - loss: 17785.7158 - val_loss: 30073.1446\n",
      "Epoch 319/1000\n",
      "1168/1168 [==============================] - 0s 111us/step - loss: 17403.4362 - val_loss: 30103.6153\n",
      "Epoch 320/1000\n",
      "1168/1168 [==============================] - 0s 134us/step - loss: 17233.4481 - val_loss: 30077.2256\n",
      "Epoch 321/1000\n",
      "1168/1168 [==============================] - 0s 116us/step - loss: 17709.8789 - val_loss: 30135.1714\n",
      "Epoch 322/1000\n",
      "1168/1168 [==============================] - 0s 178us/step - loss: 17629.3927 - val_loss: 30482.4768\n",
      "Epoch 323/1000\n",
      "1168/1168 [==============================] - 0s 111us/step - loss: 17659.4079 - val_loss: 30268.5822\n",
      "Epoch 324/1000\n",
      "1168/1168 [==============================] - 0s 146us/step - loss: 17687.2759 - val_loss: 30408.3694\n",
      "Epoch 325/1000\n",
      "1168/1168 [==============================] - 0s 125us/step - loss: 17565.3144 - val_loss: 30624.5069\n",
      "Epoch 326/1000\n",
      "1168/1168 [==============================] - 0s 140us/step - loss: 17478.8449 - val_loss: 30086.3445\n",
      "Epoch 327/1000\n",
      "1168/1168 [==============================] - 0s 116us/step - loss: 17361.5376 - val_loss: 30167.0529\n",
      "Epoch 328/1000\n",
      "1168/1168 [==============================] - 0s 173us/step - loss: 17664.7896 - val_loss: 30160.8014\n",
      "Epoch 329/1000\n",
      "1168/1168 [==============================] - 0s 133us/step - loss: 17552.6334 - val_loss: 30058.3675\n",
      "Epoch 330/1000\n",
      "1168/1168 [==============================] - 0s 108us/step - loss: 17393.6812 - val_loss: 30099.6059\n",
      "Epoch 331/1000\n",
      "1168/1168 [==============================] - 0s 127us/step - loss: 17474.3981 - val_loss: 30211.1394\n",
      "Epoch 332/1000\n",
      "1168/1168 [==============================] - 0s 105us/step - loss: 17184.1121 - val_loss: 30216.1148\n",
      "Epoch 333/1000\n",
      "1168/1168 [==============================] - 0s 128us/step - loss: 17376.4919 - val_loss: 30128.4788\n",
      "Epoch 334/1000\n",
      "1168/1168 [==============================] - 0s 111us/step - loss: 17439.3385 - val_loss: 30069.6761\n",
      "Epoch 335/1000\n",
      "1168/1168 [==============================] - 0s 181us/step - loss: 17365.1527 - val_loss: 30369.3059\n",
      "Epoch 336/1000\n",
      "1168/1168 [==============================] - 0s 117us/step - loss: 17379.2107 - val_loss: 30285.3506\n",
      "Epoch 337/1000\n",
      "1168/1168 [==============================] - 0s 148us/step - loss: 17403.0322 - val_loss: 30108.9790\n",
      "Epoch 338/1000\n",
      "1168/1168 [==============================] - 0s 130us/step - loss: 17530.2142 - val_loss: 30554.3019\n",
      "Epoch 339/1000\n",
      "1168/1168 [==============================] - 0s 133us/step - loss: 17459.5096 - val_loss: 30233.9945\n",
      "Epoch 340/1000\n",
      "1168/1168 [==============================] - 0s 109us/step - loss: 17535.7099 - val_loss: 30190.9020\n",
      "Epoch 341/1000\n",
      "1168/1168 [==============================] - 0s 140us/step - loss: 17195.3094 - val_loss: 30480.4003\n",
      "Epoch 342/1000\n",
      "1168/1168 [==============================] - 0s 118us/step - loss: 17280.1233 - val_loss: 30441.0738\n",
      "Epoch 343/1000\n",
      "1168/1168 [==============================] - 0s 121us/step - loss: 17225.8044 - val_loss: 30465.9609\n",
      "Epoch 344/1000\n",
      "1168/1168 [==============================] - 0s 132us/step - loss: 17345.7382 - val_loss: 30208.8696\n",
      "Epoch 345/1000\n",
      "1168/1168 [==============================] - 0s 109us/step - loss: 16953.1975 - val_loss: 30258.3297\n",
      "Epoch 346/1000\n",
      "1168/1168 [==============================] - 0s 120us/step - loss: 17372.6026 - val_loss: 30349.4446\n",
      "Epoch 347/1000\n",
      "1168/1168 [==============================] - 0s 176us/step - loss: 17227.9658 - val_loss: 30330.4687\n",
      "Epoch 348/1000\n",
      "1168/1168 [==============================] - 0s 178us/step - loss: 17120.1257 - val_loss: 30248.0724\n",
      "Epoch 349/1000\n",
      "1168/1168 [==============================] - 0s 133us/step - loss: 17322.4755 - val_loss: 30238.6028\n",
      "Epoch 350/1000\n",
      "1168/1168 [==============================] - 0s 134us/step - loss: 17058.1685 - val_loss: 30200.9846\n",
      "Epoch 351/1000\n",
      "1168/1168 [==============================] - 0s 133us/step - loss: 17191.7568 - val_loss: 30203.6181\n",
      "Epoch 352/1000\n",
      "1168/1168 [==============================] - 0s 119us/step - loss: 17232.2534 - val_loss: 30223.6381\n",
      "Epoch 353/1000\n",
      "1168/1168 [==============================] - 0s 133us/step - loss: 17357.6034 - val_loss: 30173.7189\n",
      "Epoch 354/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1168/1168 [==============================] - 0s 124us/step - loss: 17003.1228 - val_loss: 30199.5568\n",
      "Epoch 355/1000\n",
      "1168/1168 [==============================] - 0s 115us/step - loss: 17109.2985 - val_loss: 30216.0535\n",
      "Epoch 356/1000\n",
      "1168/1168 [==============================] - 0s 117us/step - loss: 17264.0905 - val_loss: 30325.7078\n",
      "Epoch 357/1000\n",
      "1168/1168 [==============================] - 0s 113us/step - loss: 17438.1559 - val_loss: 30291.0326\n",
      "Epoch 358/1000\n",
      "1168/1168 [==============================] - 0s 128us/step - loss: 16971.4064 - val_loss: 30216.8447\n",
      "Epoch 359/1000\n",
      "1168/1168 [==============================] - 0s 99us/step - loss: 17153.7062 - val_loss: 30225.6817\n",
      "Epoch 360/1000\n",
      "1168/1168 [==============================] - 0s 111us/step - loss: 17176.2690 - val_loss: 30212.7406\n",
      "Epoch 361/1000\n",
      "1168/1168 [==============================] - 0s 170us/step - loss: 17355.7614 - val_loss: 30244.1534\n",
      "Epoch 362/1000\n",
      "1168/1168 [==============================] - 0s 100us/step - loss: 17217.4000 - val_loss: 30623.7417\n",
      "Epoch 363/1000\n",
      "1168/1168 [==============================] - 0s 109us/step - loss: 16963.1998 - val_loss: 30277.4729\n",
      "Epoch 364/1000\n",
      "1168/1168 [==============================] - 0s 120us/step - loss: 17215.2490 - val_loss: 30265.1373\n",
      "Epoch 365/1000\n",
      "1168/1168 [==============================] - 0s 107us/step - loss: 17185.2237 - val_loss: 30214.8199\n",
      "Epoch 366/1000\n",
      "1168/1168 [==============================] - 0s 120us/step - loss: 17083.6830 - val_loss: 30255.4025\n",
      "Epoch 367/1000\n",
      "1168/1168 [==============================] - 0s 117us/step - loss: 16762.4230 - val_loss: 30686.7267\n",
      "Epoch 368/1000\n",
      "1168/1168 [==============================] - 0s 165us/step - loss: 16976.4678 - val_loss: 30260.1378\n",
      "Epoch 369/1000\n",
      "1168/1168 [==============================] - 0s 115us/step - loss: 17074.3548 - val_loss: 30428.2360\n",
      "Epoch 370/1000\n",
      "1168/1168 [==============================] - 0s 115us/step - loss: 16937.6306 - val_loss: 30335.8545\n",
      "Epoch 371/1000\n",
      "1168/1168 [==============================] - 0s 118us/step - loss: 17122.4989 - val_loss: 30290.3690\n",
      "Epoch 372/1000\n",
      "1168/1168 [==============================] - 0s 100us/step - loss: 16993.1764 - val_loss: 30452.2732\n",
      "Epoch 373/1000\n",
      "1168/1168 [==============================] - 0s 112us/step - loss: 17011.2396 - val_loss: 30356.2779\n",
      "Epoch 374/1000\n",
      "1168/1168 [==============================] - 0s 122us/step - loss: 16859.5107 - val_loss: 30332.7753\n",
      "Epoch 375/1000\n",
      "1168/1168 [==============================] - 0s 176us/step - loss: 16746.9447 - val_loss: 30317.5470\n",
      "Epoch 376/1000\n",
      "1168/1168 [==============================] - 0s 112us/step - loss: 16810.4583 - val_loss: 30335.2523\n",
      "Epoch 377/1000\n",
      "1168/1168 [==============================] - 0s 117us/step - loss: 17079.2096 - val_loss: 30321.9577\n",
      "Epoch 378/1000\n",
      "1168/1168 [==============================] - 0s 121us/step - loss: 17013.9089 - val_loss: 30296.8427\n",
      "Epoch 379/1000\n",
      "1168/1168 [==============================] - 0s 102us/step - loss: 16998.2128 - val_loss: 30358.3673\n",
      "Epoch 380/1000\n",
      "1168/1168 [==============================] - 0s 102us/step - loss: 16905.0134 - val_loss: 30721.7645\n",
      "Epoch 381/1000\n",
      "1168/1168 [==============================] - 0s 124us/step - loss: 17088.6375 - val_loss: 30416.2459\n",
      "Epoch 382/1000\n",
      "1168/1168 [==============================] - 0s 126us/step - loss: 17124.6749 - val_loss: 30605.1510\n",
      "Epoch 383/1000\n",
      "1168/1168 [==============================] - 0s 104us/step - loss: 16970.7348 - val_loss: 30338.8436\n",
      "Epoch 384/1000\n",
      "1168/1168 [==============================] - 0s 122us/step - loss: 17007.1513 - val_loss: 30351.1513\n",
      "Epoch 385/1000\n",
      "1168/1168 [==============================] - 0s 114us/step - loss: 16814.0464 - val_loss: 30392.2209\n",
      "Epoch 386/1000\n",
      "1168/1168 [==============================] - 0s 113us/step - loss: 16462.7104 - val_loss: 30354.4610\n",
      "Epoch 387/1000\n",
      "1168/1168 [==============================] - 0s 119us/step - loss: 16871.0875 - val_loss: 30386.2487\n",
      "Epoch 388/1000\n",
      "1168/1168 [==============================] - 0s 103us/step - loss: 17030.3062 - val_loss: 30427.4240\n",
      "Epoch 389/1000\n",
      "1168/1168 [==============================] - 0s 164us/step - loss: 16651.0410 - val_loss: 30616.4138\n",
      "Epoch 390/1000\n",
      "1168/1168 [==============================] - 0s 118us/step - loss: 16904.0142 - val_loss: 30450.2997\n",
      "Epoch 391/1000\n",
      "1168/1168 [==============================] - 0s 125us/step - loss: 16623.5700 - val_loss: 30384.7466\n",
      "Epoch 392/1000\n",
      "1168/1168 [==============================] - 0s 116us/step - loss: 16636.8669 - val_loss: 30662.9153\n",
      "Epoch 393/1000\n",
      "1168/1168 [==============================] - 0s 114us/step - loss: 16635.9027 - val_loss: 30721.1645\n",
      "Epoch 394/1000\n",
      "1168/1168 [==============================] - 0s 119us/step - loss: 16532.6997 - val_loss: 30426.7219\n",
      "Epoch 395/1000\n",
      "1168/1168 [==============================] - 0s 100us/step - loss: 16900.6216 - val_loss: 30534.7841\n",
      "Epoch 396/1000\n",
      "1168/1168 [==============================] - 0s 165us/step - loss: 16748.6106 - val_loss: 30463.5905\n",
      "Epoch 397/1000\n",
      "1168/1168 [==============================] - 0s 116us/step - loss: 16504.8490 - val_loss: 30478.4387\n",
      "Epoch 398/1000\n",
      "1168/1168 [==============================] - 0s 132us/step - loss: 16829.9441 - val_loss: 30514.2917\n",
      "Epoch 399/1000\n",
      "1168/1168 [==============================] - 0s 105us/step - loss: 16684.2312 - val_loss: 30450.8188\n",
      "Epoch 400/1000\n",
      "1168/1168 [==============================] - 0s 112us/step - loss: 16697.1832 - val_loss: 30996.2933\n",
      "Epoch 401/1000\n",
      "1168/1168 [==============================] - 0s 125us/step - loss: 16784.3021 - val_loss: 30469.4452\n",
      "Epoch 402/1000\n",
      "1168/1168 [==============================] - 0s 117us/step - loss: 16680.4939 - val_loss: 30563.5568\n",
      "Epoch 403/1000\n",
      "1168/1168 [==============================] - 0s 149us/step - loss: 16795.2368 - val_loss: 30469.0079\n",
      "Epoch 404/1000\n",
      "1168/1168 [==============================] - 0s 119us/step - loss: 16561.3480 - val_loss: 30506.0993\n",
      "Epoch 405/1000\n",
      "1168/1168 [==============================] - 0s 112us/step - loss: 16793.4496 - val_loss: 30887.6147\n",
      "Epoch 406/1000\n",
      "1168/1168 [==============================] - 0s 127us/step - loss: 16718.6899 - val_loss: 30516.6185\n",
      "Epoch 407/1000\n",
      "1168/1168 [==============================] - 0s 103us/step - loss: 16647.7098 - val_loss: 30440.4973\n",
      "Epoch 408/1000\n",
      "1168/1168 [==============================] - 0s 111us/step - loss: 16488.5176 - val_loss: 30510.4048\n",
      "Epoch 409/1000\n",
      "1168/1168 [==============================] - 0s 123us/step - loss: 16633.8915 - val_loss: 30500.3914\n",
      "Epoch 410/1000\n",
      "1168/1168 [==============================] - 0s 170us/step - loss: 16660.1585 - val_loss: 30504.7161\n",
      "Epoch 411/1000\n",
      "1168/1168 [==============================] - 0s 115us/step - loss: 16707.8593 - val_loss: 30490.2910\n",
      "Epoch 412/1000\n",
      "1168/1168 [==============================] - 0s 131us/step - loss: 16029.3922 - val_loss: 30806.7469\n",
      "Epoch 413/1000\n",
      "1168/1168 [==============================] - 0s 103us/step - loss: 16378.7850 - val_loss: 30571.0053\n",
      "Epoch 414/1000\n",
      "1168/1168 [==============================] - 0s 111us/step - loss: 16628.0553 - val_loss: 30569.4382\n",
      "Epoch 415/1000\n",
      "1168/1168 [==============================] - 0s 101us/step - loss: 16422.3005 - val_loss: 30552.2603\n",
      "Epoch 416/1000\n",
      "1168/1168 [==============================] - 0s 106us/step - loss: 16579.5578 - val_loss: 30572.2480\n",
      "Epoch 417/1000\n",
      "1168/1168 [==============================] - 0s 148us/step - loss: 16409.3227 - val_loss: 30539.0448\n",
      "Epoch 418/1000\n",
      "1168/1168 [==============================] - 0s 124us/step - loss: 16676.6714 - val_loss: 30540.7282\n",
      "Epoch 419/1000\n",
      "1168/1168 [==============================] - 0s 118us/step - loss: 16687.4421 - val_loss: 30796.4590\n",
      "Epoch 420/1000\n",
      "1168/1168 [==============================] - 0s 125us/step - loss: 16438.1903 - val_loss: 30635.0930\n",
      "Epoch 421/1000\n",
      "1168/1168 [==============================] - 0s 106us/step - loss: 16626.8061 - val_loss: 30643.8499\n",
      "Epoch 422/1000\n",
      "1168/1168 [==============================] - 0s 110us/step - loss: 16366.3090 - val_loss: 30685.4661\n",
      "Epoch 423/1000\n",
      "1168/1168 [==============================] - 0s 140us/step - loss: 16388.5872 - val_loss: 30620.6727\n",
      "Epoch 424/1000\n",
      "1168/1168 [==============================] - 0s 142us/step - loss: 16516.6166 - val_loss: 30572.3369\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 425/1000\n",
      "1168/1168 [==============================] - 0s 116us/step - loss: 16591.3510 - val_loss: 31169.5799\n",
      "Epoch 426/1000\n",
      "1168/1168 [==============================] - 0s 116us/step - loss: 16614.9666 - val_loss: 30812.1211\n",
      "Epoch 427/1000\n",
      "1168/1168 [==============================] - 0s 116us/step - loss: 16551.0056 - val_loss: 30567.7083\n",
      "Epoch 428/1000\n",
      "1168/1168 [==============================] - 0s 117us/step - loss: 16257.4820 - val_loss: 30567.1801\n",
      "Epoch 429/1000\n",
      "1168/1168 [==============================] - 0s 116us/step - loss: 16276.8565 - val_loss: 30636.8985\n",
      "Epoch 430/1000\n",
      "1168/1168 [==============================] - 0s 129us/step - loss: 16594.9520 - val_loss: 30584.0282\n",
      "Epoch 431/1000\n",
      "1168/1168 [==============================] - 0s 166us/step - loss: 16532.5194 - val_loss: 30622.5854\n",
      "Epoch 432/1000\n",
      "1168/1168 [==============================] - 0s 116us/step - loss: 16491.0682 - val_loss: 30582.4192\n",
      "Epoch 433/1000\n",
      "1168/1168 [==============================] - 0s 119us/step - loss: 16316.9701 - val_loss: 30576.5339\n",
      "Epoch 434/1000\n",
      "1168/1168 [==============================] - 0s 104us/step - loss: 16460.2770 - val_loss: 30622.3053\n",
      "Epoch 435/1000\n",
      "1168/1168 [==============================] - 0s 117us/step - loss: 16505.7590 - val_loss: 30675.7976\n",
      "Epoch 436/1000\n",
      "1168/1168 [==============================] - 0s 117us/step - loss: 16358.3798 - val_loss: 30633.3546\n",
      "Epoch 437/1000\n",
      "1168/1168 [==============================] - 0s 109us/step - loss: 16408.5086 - val_loss: 30632.8760\n",
      "Epoch 438/1000\n",
      "1168/1168 [==============================] - 0s 161us/step - loss: 16456.9669 - val_loss: 30819.8837\n",
      "Epoch 439/1000\n",
      "1168/1168 [==============================] - 0s 130us/step - loss: 16222.4113 - val_loss: 30780.6991\n",
      "Epoch 440/1000\n",
      "1168/1168 [==============================] - 0s 105us/step - loss: 15972.0284 - val_loss: 30659.8349\n",
      "Epoch 441/1000\n",
      "1168/1168 [==============================] - 0s 121us/step - loss: 16342.9913 - val_loss: 30988.8938\n",
      "Epoch 442/1000\n",
      "1168/1168 [==============================] - 0s 120us/step - loss: 16351.6217 - val_loss: 30723.5524\n",
      "Epoch 443/1000\n",
      "1168/1168 [==============================] - 0s 126us/step - loss: 16505.9766 - val_loss: 30677.7277\n",
      "Epoch 444/1000\n",
      "1168/1168 [==============================] - 0s 148us/step - loss: 16211.3245 - val_loss: 30686.6102\n",
      "Epoch 445/1000\n",
      "1168/1168 [==============================] - 0s 114us/step - loss: 16299.4813 - val_loss: 30676.9055\n",
      "Epoch 446/1000\n",
      "1168/1168 [==============================] - 0s 117us/step - loss: 16284.1841 - val_loss: 30668.6677\n",
      "Epoch 447/1000\n",
      "1168/1168 [==============================] - 0s 116us/step - loss: 16148.2138 - val_loss: 31037.3957\n",
      "Epoch 448/1000\n",
      "1168/1168 [==============================] - 0s 122us/step - loss: 16252.5543 - val_loss: 30818.8679\n",
      "Epoch 449/1000\n",
      "1168/1168 [==============================] - 0s 117us/step - loss: 16410.1916 - val_loss: 30980.0130\n",
      "Epoch 450/1000\n",
      "1168/1168 [==============================] - 0s 103us/step - loss: 16261.4561 - val_loss: 30857.2721\n",
      "Epoch 451/1000\n",
      "1168/1168 [==============================] - 0s 112us/step - loss: 16177.7943 - val_loss: 30820.2676\n",
      "Epoch 452/1000\n",
      "1168/1168 [==============================] - 0s 159us/step - loss: 16198.5788 - val_loss: 30669.3482\n",
      "Epoch 453/1000\n",
      "1168/1168 [==============================] - 0s 115us/step - loss: 16104.8813 - val_loss: 30866.0802\n",
      "Epoch 454/1000\n",
      "1168/1168 [==============================] - 0s 111us/step - loss: 16363.3244 - val_loss: 30718.1267\n",
      "Epoch 455/1000\n",
      "1168/1168 [==============================] - 0s 115us/step - loss: 16138.2483 - val_loss: 30807.9638\n",
      "Epoch 456/1000\n",
      "1168/1168 [==============================] - 0s 117us/step - loss: 16084.5209 - val_loss: 30811.2039\n",
      "Epoch 457/1000\n",
      "1168/1168 [==============================] - 0s 120us/step - loss: 16227.2537 - val_loss: 30716.7596\n",
      "Epoch 458/1000\n",
      "1168/1168 [==============================] - 0s 133us/step - loss: 16038.6170 - val_loss: 30794.4922\n",
      "Epoch 459/1000\n",
      "1168/1168 [==============================] - 0s 118us/step - loss: 16044.8472 - val_loss: 30763.5707\n",
      "Epoch 460/1000\n",
      "1168/1168 [==============================] - 0s 118us/step - loss: 16258.0557 - val_loss: 30749.3377\n",
      "Epoch 461/1000\n",
      "1168/1168 [==============================] - 0s 118us/step - loss: 16138.0055 - val_loss: 30959.0700\n",
      "Epoch 462/1000\n",
      "1168/1168 [==============================] - 0s 123us/step - loss: 16160.8577 - val_loss: 30788.4944\n",
      "Epoch 463/1000\n",
      "1168/1168 [==============================] - 0s 109us/step - loss: 16212.1534 - val_loss: 30770.8283\n",
      "Epoch 464/1000\n",
      "1168/1168 [==============================] - 0s 111us/step - loss: 16159.3749 - val_loss: 30751.3059\n",
      "Epoch 465/1000\n",
      "1168/1168 [==============================] - 0s 114us/step - loss: 16167.8245 - val_loss: 30850.0002\n",
      "Epoch 466/1000\n",
      "1168/1168 [==============================] - 0s 169us/step - loss: 15929.0421 - val_loss: 30758.3378\n",
      "Epoch 467/1000\n",
      "1168/1168 [==============================] - 0s 116us/step - loss: 16286.4028 - val_loss: 30746.1588\n",
      "Epoch 468/1000\n",
      "1168/1168 [==============================] - 0s 116us/step - loss: 16114.4513 - val_loss: 30779.1877\n",
      "Epoch 469/1000\n",
      "1168/1168 [==============================] - 0s 105us/step - loss: 16109.2650 - val_loss: 30759.4183\n",
      "Epoch 470/1000\n",
      "1168/1168 [==============================] - 0s 114us/step - loss: 15975.2810 - val_loss: 30907.2032\n",
      "Epoch 471/1000\n",
      "1168/1168 [==============================] - 0s 117us/step - loss: 16114.5526 - val_loss: 30924.9950\n",
      "Epoch 472/1000\n",
      "1168/1168 [==============================] - 0s 140us/step - loss: 16040.3667 - val_loss: 30836.6920\n",
      "Epoch 473/1000\n",
      "1168/1168 [==============================] - 0s 144us/step - loss: 15912.2409 - val_loss: 30799.7770\n",
      "Epoch 474/1000\n",
      "1168/1168 [==============================] - 0s 118us/step - loss: 16078.6460 - val_loss: 31378.8169\n",
      "Epoch 475/1000\n",
      "1168/1168 [==============================] - 0s 113us/step - loss: 16013.6910 - val_loss: 30797.8706\n",
      "Epoch 476/1000\n",
      "1168/1168 [==============================] - 0s 139us/step - loss: 15777.5129 - val_loss: 30837.0158\n",
      "Epoch 477/1000\n",
      "1168/1168 [==============================] - 0s 123us/step - loss: 16085.5022 - val_loss: 31224.3557\n",
      "Epoch 478/1000\n",
      "1168/1168 [==============================] - 0s 107us/step - loss: 15983.8947 - val_loss: 30843.9740\n",
      "Epoch 479/1000\n",
      "1168/1168 [==============================] - 0s 176us/step - loss: 16059.0828 - val_loss: 30831.9945\n",
      "Epoch 480/1000\n",
      "1168/1168 [==============================] - 0s 125us/step - loss: 16007.2367 - val_loss: 31188.1425\n",
      "Epoch 481/1000\n",
      "1168/1168 [==============================] - 0s 126us/step - loss: 15959.5211 - val_loss: 31107.0327\n",
      "Epoch 482/1000\n",
      "1168/1168 [==============================] - 0s 112us/step - loss: 15807.4949 - val_loss: 30930.9560\n",
      "Epoch 483/1000\n",
      "1168/1168 [==============================] - 0s 109us/step - loss: 16224.4399 - val_loss: 30901.1879\n",
      "Epoch 484/1000\n",
      "1168/1168 [==============================] - 0s 115us/step - loss: 15912.3543 - val_loss: 30931.3152\n",
      "Epoch 485/1000\n",
      "1168/1168 [==============================] - 0s 114us/step - loss: 15865.3878 - val_loss: 30935.9325\n",
      "Epoch 486/1000\n",
      "1168/1168 [==============================] - 0s 159us/step - loss: 15869.1148 - val_loss: 30940.0588\n",
      "Epoch 487/1000\n",
      "1168/1168 [==============================] - 0s 109us/step - loss: 15901.0905 - val_loss: 30995.3349\n",
      "Epoch 488/1000\n",
      "1168/1168 [==============================] - 0s 128us/step - loss: 15897.7506 - val_loss: 30933.1205\n",
      "Epoch 489/1000\n",
      "1168/1168 [==============================] - 0s 123us/step - loss: 15720.0912 - val_loss: 31606.7911\n",
      "Epoch 490/1000\n",
      "1168/1168 [==============================] - 0s 117us/step - loss: 15981.3075 - val_loss: 30940.8060\n",
      "Epoch 491/1000\n",
      "1168/1168 [==============================] - 0s 113us/step - loss: 15889.5786 - val_loss: 31111.0278\n",
      "Epoch 492/1000\n",
      "1168/1168 [==============================] - 0s 117us/step - loss: 15922.7337 - val_loss: 31028.4265\n",
      "Epoch 493/1000\n",
      "1168/1168 [==============================] - 0s 171us/step - loss: 15820.3118 - val_loss: 30923.8428\n",
      "Epoch 494/1000\n",
      "1168/1168 [==============================] - 0s 129us/step - loss: 15796.8038 - val_loss: 30951.8647\n",
      "Epoch 495/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1168/1168 [==============================] - 0s 118us/step - loss: 15888.8862 - val_loss: 30888.8464\n",
      "Epoch 496/1000\n",
      "1168/1168 [==============================] - 0s 102us/step - loss: 15960.8329 - val_loss: 31069.1945\n",
      "Epoch 497/1000\n",
      "1168/1168 [==============================] - 0s 112us/step - loss: 15839.5722 - val_loss: 31297.7265\n",
      "Epoch 498/1000\n",
      "1168/1168 [==============================] - 0s 119us/step - loss: 15642.2816 - val_loss: 31006.1060\n",
      "Epoch 499/1000\n",
      "1168/1168 [==============================] - 0s 114us/step - loss: 15651.4890 - val_loss: 31041.4509\n",
      "Epoch 500/1000\n",
      "1168/1168 [==============================] - 0s 157us/step - loss: 15954.2092 - val_loss: 31053.9030\n",
      "Epoch 501/1000\n",
      "1168/1168 [==============================] - 0s 126us/step - loss: 15992.3352 - val_loss: 31023.5101\n",
      "Epoch 502/1000\n",
      "1168/1168 [==============================] - 0s 110us/step - loss: 15754.2862 - val_loss: 31136.4700\n",
      "Epoch 503/1000\n",
      "1168/1168 [==============================] - 0s 129us/step - loss: 15768.2985 - val_loss: 31007.1562\n",
      "Epoch 504/1000\n",
      "1168/1168 [==============================] - 0s 105us/step - loss: 15661.0696 - val_loss: 31652.8824\n",
      "Epoch 505/1000\n",
      "1168/1168 [==============================] - 0s 118us/step - loss: 16082.1923 - val_loss: 30997.2573\n",
      "Epoch 506/1000\n",
      "1168/1168 [==============================] - 0s 115us/step - loss: 15927.6683 - val_loss: 31199.2638\n",
      "Epoch 507/1000\n",
      "1168/1168 [==============================] - 0s 145us/step - loss: 15725.7215 - val_loss: 31400.3636\n",
      "Epoch 508/1000\n",
      "1168/1168 [==============================] - 0s 121us/step - loss: 15707.3962 - val_loss: 31205.5204\n",
      "Epoch 509/1000\n",
      "1168/1168 [==============================] - 0s 116us/step - loss: 15668.0171 - val_loss: 31038.7898\n",
      "Epoch 510/1000\n",
      "1168/1168 [==============================] - 0s 116us/step - loss: 15693.5835 - val_loss: 31108.2899\n",
      "Epoch 511/1000\n",
      "1168/1168 [==============================] - 0s 115us/step - loss: 15929.5176 - val_loss: 31015.9094\n",
      "Epoch 512/1000\n",
      "1168/1168 [==============================] - 0s 121us/step - loss: 15837.7544 - val_loss: 31033.3686\n",
      "Epoch 513/1000\n",
      "1168/1168 [==============================] - 0s 115us/step - loss: 15739.8485 - val_loss: 31062.8685\n",
      "Epoch 514/1000\n",
      "1168/1168 [==============================] - 0s 171us/step - loss: 15736.3686 - val_loss: 31004.3782\n",
      "Epoch 515/1000\n",
      "1168/1168 [==============================] - 0s 103us/step - loss: 15868.8131 - val_loss: 31045.6980\n",
      "Epoch 516/1000\n",
      "1168/1168 [==============================] - 0s 116us/step - loss: 15707.5344 - val_loss: 31182.4577\n",
      "Epoch 517/1000\n",
      "1168/1168 [==============================] - 0s 122us/step - loss: 15600.5166 - val_loss: 31355.4412\n",
      "Epoch 518/1000\n",
      "1168/1168 [==============================] - 0s 115us/step - loss: 15554.1989 - val_loss: 31217.3170\n",
      "Epoch 519/1000\n",
      "1168/1168 [==============================] - 0s 110us/step - loss: 15613.1960 - val_loss: 31136.5862\n",
      "Epoch 520/1000\n",
      "1168/1168 [==============================] - 0s 117us/step - loss: 15752.4766 - val_loss: 31088.2553\n",
      "Epoch 521/1000\n",
      "1168/1168 [==============================] - 0s 118us/step - loss: 15381.1015 - val_loss: 31232.8103\n",
      "Epoch 522/1000\n",
      "1168/1168 [==============================] - 0s 118us/step - loss: 15483.9328 - val_loss: 31102.9822\n",
      "Epoch 523/1000\n",
      "1168/1168 [==============================] - 0s 108us/step - loss: 15876.2594 - val_loss: 31112.3432\n",
      "Epoch 524/1000\n",
      "1168/1168 [==============================] - 0s 114us/step - loss: 15556.0910 - val_loss: 31188.6316\n",
      "Epoch 525/1000\n",
      "1168/1168 [==============================] - 0s 117us/step - loss: 15471.8893 - val_loss: 31291.9279\n",
      "Epoch 526/1000\n",
      "1168/1168 [==============================] - 0s 103us/step - loss: 15424.6360 - val_loss: 31164.1115\n",
      "Epoch 527/1000\n",
      "1168/1168 [==============================] - 0s 108us/step - loss: 15661.0162 - val_loss: 31138.1155\n",
      "Epoch 528/1000\n",
      "1168/1168 [==============================] - 0s 160us/step - loss: 15660.1233 - val_loss: 31431.9629\n",
      "Epoch 529/1000\n",
      "1168/1168 [==============================] - 0s 111us/step - loss: 15646.2058 - val_loss: 31205.7090\n",
      "Epoch 530/1000\n",
      "1168/1168 [==============================] - 0s 114us/step - loss: 15454.7062 - val_loss: 31329.2407\n",
      "Epoch 531/1000\n",
      "1168/1168 [==============================] - 0s 119us/step - loss: 15512.0645 - val_loss: 31149.9509\n",
      "Epoch 532/1000\n",
      "1168/1168 [==============================] - 0s 120us/step - loss: 15452.1676 - val_loss: 31288.1869\n",
      "Epoch 533/1000\n",
      "1168/1168 [==============================] - 0s 109us/step - loss: 15449.7323 - val_loss: 31264.5511\n",
      "Epoch 534/1000\n",
      "1168/1168 [==============================] - 0s 109us/step - loss: 15545.6883 - val_loss: 31161.3192\n",
      "Epoch 535/1000\n",
      "1168/1168 [==============================] - 0s 163us/step - loss: 15379.8826 - val_loss: 31285.0497\n",
      "Epoch 536/1000\n",
      "1168/1168 [==============================] - 0s 124us/step - loss: 15269.5055 - val_loss: 31169.6691\n",
      "Epoch 537/1000\n",
      "1168/1168 [==============================] - 0s 100us/step - loss: 15467.9173 - val_loss: 31246.5924\n",
      "Epoch 538/1000\n",
      "1168/1168 [==============================] - 0s 112us/step - loss: 15531.6773 - val_loss: 31361.8223\n",
      "Epoch 539/1000\n",
      "1168/1168 [==============================] - 0s 120us/step - loss: 15478.8288 - val_loss: 31169.7702\n",
      "Epoch 540/1000\n",
      "1168/1168 [==============================] - 0s 118us/step - loss: 15616.9424 - val_loss: 31198.3204\n",
      "Epoch 541/1000\n",
      "1168/1168 [==============================] - 0s 117us/step - loss: 15531.9780 - val_loss: 31167.6833\n",
      "Epoch 542/1000\n",
      "1168/1168 [==============================] - 0s 169us/step - loss: 15416.0664 - val_loss: 31160.9816\n",
      "Epoch 543/1000\n",
      "1168/1168 [==============================] - 0s 100us/step - loss: 15560.5987 - val_loss: 31231.6668\n",
      "Epoch 544/1000\n",
      "1168/1168 [==============================] - 0s 119us/step - loss: 15513.9746 - val_loss: 31287.2544\n",
      "Epoch 545/1000\n",
      "1168/1168 [==============================] - 0s 123us/step - loss: 15490.0085 - val_loss: 31169.6034\n",
      "Epoch 546/1000\n",
      "1168/1168 [==============================] - 0s 109us/step - loss: 15477.7353 - val_loss: 31212.1164\n",
      "Epoch 547/1000\n",
      "1168/1168 [==============================] - 0s 130us/step - loss: 15322.7669 - val_loss: 31396.6982\n",
      "Epoch 548/1000\n",
      "1168/1168 [==============================] - 0s 106us/step - loss: 15588.3209 - val_loss: 31318.1887\n",
      "Epoch 549/1000\n",
      "1168/1168 [==============================] - 0s 148us/step - loss: 15480.2258 - val_loss: 31183.3548\n",
      "Epoch 550/1000\n",
      "1168/1168 [==============================] - 0s 123us/step - loss: 15397.8008 - val_loss: 31572.6549\n",
      "Epoch 551/1000\n",
      "1168/1168 [==============================] - 0s 109us/step - loss: 15389.1079 - val_loss: 31333.3606\n",
      "Epoch 552/1000\n",
      "1168/1168 [==============================] - 0s 125us/step - loss: 15362.9884 - val_loss: 31242.2075\n",
      "Epoch 553/1000\n",
      "1168/1168 [==============================] - 0s 121us/step - loss: 15510.0337 - val_loss: 31251.1382\n",
      "Epoch 554/1000\n",
      "1168/1168 [==============================] - 0s 122us/step - loss: 15483.5446 - val_loss: 31388.8519\n",
      "Epoch 555/1000\n",
      "1168/1168 [==============================] - 0s 110us/step - loss: 15443.7164 - val_loss: 31307.9216\n",
      "Epoch 556/1000\n",
      "1168/1168 [==============================] - 0s 180us/step - loss: 15505.1708 - val_loss: 31637.7019\n",
      "Epoch 557/1000\n",
      "1168/1168 [==============================] - 0s 118us/step - loss: 15325.4041 - val_loss: 31629.0170\n",
      "Epoch 558/1000\n",
      "1168/1168 [==============================] - 0s 119us/step - loss: 15337.2018 - val_loss: 31268.8887\n",
      "Epoch 559/1000\n",
      "1168/1168 [==============================] - 0s 116us/step - loss: 15341.6180 - val_loss: 31307.5883\n",
      "Epoch 560/1000\n",
      "1168/1168 [==============================] - 0s 110us/step - loss: 15388.9676 - val_loss: 31310.0830\n",
      "Epoch 561/1000\n",
      "1168/1168 [==============================] - 0s 105us/step - loss: 15262.9658 - val_loss: 31389.3747\n",
      "Epoch 562/1000\n",
      "1168/1168 [==============================] - 0s 122us/step - loss: 15447.4459 - val_loss: 32623.7910\n",
      "Epoch 563/1000\n",
      "1168/1168 [==============================] - 0s 153us/step - loss: 15367.4918 - val_loss: 31321.1462\n",
      "Epoch 564/1000\n",
      "1168/1168 [==============================] - 0s 131us/step - loss: 15254.6594 - val_loss: 31737.1794\n",
      "Epoch 565/1000\n",
      "1168/1168 [==============================] - 0s 122us/step - loss: 15327.0728 - val_loss: 31555.3880\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 566/1000\n",
      "1168/1168 [==============================] - 0s 107us/step - loss: 15202.7355 - val_loss: 31297.7272\n",
      "Epoch 567/1000\n",
      "1168/1168 [==============================] - 0s 117us/step - loss: 15116.1058 - val_loss: 31334.2025\n",
      "Epoch 568/1000\n",
      "1168/1168 [==============================] - 0s 112us/step - loss: 15283.0021 - val_loss: 31518.0317\n",
      "Epoch 569/1000\n",
      "1168/1168 [==============================] - 0s 159us/step - loss: 15237.3781 - val_loss: 31382.1942\n",
      "Epoch 570/1000\n",
      "1168/1168 [==============================] - 0s 135us/step - loss: 15265.8439 - val_loss: 31419.3377\n",
      "Epoch 571/1000\n",
      "1168/1168 [==============================] - 0s 111us/step - loss: 15516.5221 - val_loss: 31479.5400\n",
      "Epoch 572/1000\n",
      "1168/1168 [==============================] - 0s 118us/step - loss: 15074.9059 - val_loss: 31340.8755\n",
      "Epoch 573/1000\n",
      "1168/1168 [==============================] - 0s 114us/step - loss: 15282.2036 - val_loss: 31356.2211\n",
      "Epoch 574/1000\n",
      "1168/1168 [==============================] - 0s 114us/step - loss: 14983.7655 - val_loss: 31423.2564\n",
      "Epoch 575/1000\n",
      "1168/1168 [==============================] - 0s 112us/step - loss: 15232.3497 - val_loss: 31405.0295\n",
      "Epoch 576/1000\n",
      "1168/1168 [==============================] - 0s 141us/step - loss: 15154.5685 - val_loss: 31855.9022\n",
      "Epoch 577/1000\n",
      "1168/1168 [==============================] - 0s 135us/step - loss: 15429.6682 - val_loss: 31771.5685\n",
      "Epoch 578/1000\n",
      "1168/1168 [==============================] - 0s 111us/step - loss: 15418.4858 - val_loss: 31404.2948\n",
      "Epoch 579/1000\n",
      "1168/1168 [==============================] - 0s 117us/step - loss: 15292.6507 - val_loss: 31440.9544\n",
      "Epoch 580/1000\n",
      "1168/1168 [==============================] - 0s 117us/step - loss: 15359.8442 - val_loss: 31608.5826\n",
      "Epoch 581/1000\n",
      "1168/1168 [==============================] - 0s 118us/step - loss: 15102.3189 - val_loss: 31833.0540\n",
      "Epoch 582/1000\n",
      "1168/1168 [==============================] - 0s 116us/step - loss: 15116.7317 - val_loss: 31415.2977\n",
      "Epoch 583/1000\n",
      "1168/1168 [==============================] - 0s 150us/step - loss: 15179.0615 - val_loss: 31392.4072\n",
      "Epoch 584/1000\n",
      "1168/1168 [==============================] - 0s 127us/step - loss: 15048.6914 - val_loss: 31715.5637\n",
      "Epoch 585/1000\n",
      "1168/1168 [==============================] - 0s 103us/step - loss: 15108.1165 - val_loss: 31448.0674\n",
      "Epoch 586/1000\n",
      "1168/1168 [==============================] - 0s 109us/step - loss: 14934.0859 - val_loss: 31478.9236\n",
      "Epoch 587/1000\n",
      "1168/1168 [==============================] - 0s 115us/step - loss: 15030.6826 - val_loss: 31437.1686\n",
      "Epoch 588/1000\n",
      "1168/1168 [==============================] - 0s 118us/step - loss: 14949.0781 - val_loss: 31854.8039\n",
      "Epoch 589/1000\n",
      "1168/1168 [==============================] - 0s 109us/step - loss: 15193.0318 - val_loss: 31438.9814\n",
      "Epoch 590/1000\n",
      "1168/1168 [==============================] - 0s 129us/step - loss: 15127.6293 - val_loss: 31398.1825\n",
      "Epoch 591/1000\n",
      "1168/1168 [==============================] - 0s 114us/step - loss: 14944.6552 - val_loss: 31471.2150\n",
      "Epoch 592/1000\n",
      "1168/1168 [==============================] - 0s 120us/step - loss: 15138.4224 - val_loss: 31550.2663\n",
      "Epoch 593/1000\n",
      "1168/1168 [==============================] - 0s 115us/step - loss: 15270.0336 - val_loss: 31459.2635\n",
      "Epoch 594/1000\n",
      "1168/1168 [==============================] - 0s 117us/step - loss: 15223.6758 - val_loss: 31596.2077\n",
      "Epoch 595/1000\n",
      "1168/1168 [==============================] - 0s 106us/step - loss: 15282.3232 - val_loss: 31820.7182\n",
      "Epoch 596/1000\n",
      "1168/1168 [==============================] - 0s 116us/step - loss: 15055.9427 - val_loss: 31467.7906\n",
      "Epoch 597/1000\n",
      "1168/1168 [==============================] - 0s 117us/step - loss: 15034.5381 - val_loss: 31445.0561\n",
      "Epoch 598/1000\n",
      "1168/1168 [==============================] - 0s 179us/step - loss: 15092.0975 - val_loss: 31592.7825\n",
      "Epoch 599/1000\n",
      "1168/1168 [==============================] - 0s 102us/step - loss: 15043.1671 - val_loss: 31502.3482\n",
      "Epoch 600/1000\n",
      "1168/1168 [==============================] - 0s 112us/step - loss: 15163.8697 - val_loss: 31557.3535\n",
      "Epoch 601/1000\n",
      "1168/1168 [==============================] - 0s 111us/step - loss: 15095.6101 - val_loss: 31506.2078\n",
      "Epoch 602/1000\n",
      "1168/1168 [==============================] - 0s 117us/step - loss: 15004.7806 - val_loss: 31473.2208\n",
      "Epoch 603/1000\n",
      "1168/1168 [==============================] - 0s 112us/step - loss: 15042.2190 - val_loss: 31567.7805\n",
      "Epoch 604/1000\n",
      "1168/1168 [==============================] - 0s 109us/step - loss: 15037.3070 - val_loss: 31565.0945\n",
      "Epoch 605/1000\n",
      "1168/1168 [==============================] - 0s 141us/step - loss: 14847.6716 - val_loss: 31663.7283\n",
      "Epoch 606/1000\n",
      "1168/1168 [==============================] - 0s 100us/step - loss: 15078.4015 - val_loss: 31683.4941\n",
      "Epoch 607/1000\n",
      "1168/1168 [==============================] - 0s 113us/step - loss: 14998.9792 - val_loss: 31813.6548\n",
      "Epoch 608/1000\n",
      "1168/1168 [==============================] - 0s 112us/step - loss: 15051.0117 - val_loss: 31667.5863\n",
      "Epoch 609/1000\n",
      "1168/1168 [==============================] - 0s 133us/step - loss: 14968.5036 - val_loss: 31623.0780\n",
      "Epoch 610/1000\n",
      "1168/1168 [==============================] - 0s 101us/step - loss: 15107.9039 - val_loss: 31557.5031\n",
      "Epoch 611/1000\n",
      "1168/1168 [==============================] - 0s 114us/step - loss: 14873.6047 - val_loss: 31646.8110\n",
      "Epoch 612/1000\n",
      "1168/1168 [==============================] - 0s 159us/step - loss: 14961.4692 - val_loss: 31655.6281\n",
      "Epoch 613/1000\n",
      "1168/1168 [==============================] - 0s 117us/step - loss: 14793.1363 - val_loss: 31875.0399\n",
      "Epoch 614/1000\n",
      "1168/1168 [==============================] - 0s 102us/step - loss: 15007.1322 - val_loss: 31785.1197\n",
      "Epoch 615/1000\n",
      "1168/1168 [==============================] - 0s 112us/step - loss: 15033.7464 - val_loss: 31800.9037\n",
      "Epoch 616/1000\n",
      "1168/1168 [==============================] - 0s 117us/step - loss: 14838.2583 - val_loss: 31784.5350\n",
      "Epoch 617/1000\n",
      "1168/1168 [==============================] - 0s 117us/step - loss: 14838.1348 - val_loss: 31676.6436\n",
      "Epoch 618/1000\n",
      "1168/1168 [==============================] - 0s 122us/step - loss: 15096.1604 - val_loss: 31650.7757\n",
      "Epoch 619/1000\n",
      "1168/1168 [==============================] - 0s 173us/step - loss: 15028.7853 - val_loss: 31674.5954\n",
      "Epoch 620/1000\n",
      "1168/1168 [==============================] - 0s 115us/step - loss: 15100.5798 - val_loss: 31648.0938\n",
      "Epoch 621/1000\n",
      "1168/1168 [==============================] - 0s 121us/step - loss: 14842.8667 - val_loss: 32158.6681\n",
      "Epoch 622/1000\n",
      "1168/1168 [==============================] - 0s 116us/step - loss: 14884.3175 - val_loss: 31679.6499\n",
      "Epoch 623/1000\n",
      "1168/1168 [==============================] - 0s 118us/step - loss: 14841.4359 - val_loss: 31719.7177\n",
      "Epoch 624/1000\n",
      "1168/1168 [==============================] - 0s 120us/step - loss: 14694.5601 - val_loss: 31813.7327\n",
      "Epoch 625/1000\n",
      "1168/1168 [==============================] - 0s 109us/step - loss: 14930.5424 - val_loss: 31684.5354\n",
      "Epoch 626/1000\n",
      "1168/1168 [==============================] - 0s 147us/step - loss: 14888.4879 - val_loss: 31694.1947\n",
      "Epoch 627/1000\n",
      "1168/1168 [==============================] - 0s 111us/step - loss: 14750.8359 - val_loss: 31734.8091\n",
      "Epoch 628/1000\n",
      "1168/1168 [==============================] - 0s 116us/step - loss: 14814.3248 - val_loss: 31737.9487\n",
      "Epoch 629/1000\n",
      "1168/1168 [==============================] - 0s 152us/step - loss: 14826.4876 - val_loss: 31862.1372\n",
      "Epoch 630/1000\n",
      "1168/1168 [==============================] - 0s 121us/step - loss: 14846.1405 - val_loss: 32041.9272\n",
      "Epoch 631/1000\n",
      "1168/1168 [==============================] - 0s 128us/step - loss: 14752.3482 - val_loss: 31908.7855\n",
      "Epoch 632/1000\n",
      "1168/1168 [==============================] - 0s 149us/step - loss: 14688.4684 - val_loss: 32164.5482\n",
      "Epoch 633/1000\n",
      "1168/1168 [==============================] - 0s 119us/step - loss: 15062.0106 - val_loss: 31827.0520\n",
      "Epoch 634/1000\n",
      "1168/1168 [==============================] - 0s 117us/step - loss: 14870.8183 - val_loss: 31801.2407\n",
      "Epoch 635/1000\n",
      "1168/1168 [==============================] - 0s 123us/step - loss: 15003.4928 - val_loss: 31784.4393\n",
      "Epoch 636/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1168/1168 [==============================] - 0s 129us/step - loss: 14767.2130 - val_loss: 31802.8693\n",
      "Epoch 637/1000\n",
      "1168/1168 [==============================] - 0s 113us/step - loss: 14640.0443 - val_loss: 31742.7343\n",
      "Epoch 638/1000\n",
      "1168/1168 [==============================] - 0s 122us/step - loss: 14913.5559 - val_loss: 31836.5901\n",
      "Epoch 639/1000\n",
      "1168/1168 [==============================] - 0s 156us/step - loss: 14629.2405 - val_loss: 31862.4326\n",
      "Epoch 640/1000\n",
      "1168/1168 [==============================] - 0s 120us/step - loss: 14791.6244 - val_loss: 31869.6750\n",
      "Epoch 641/1000\n",
      "1168/1168 [==============================] - 0s 111us/step - loss: 14922.6719 - val_loss: 31733.5242\n",
      "Epoch 642/1000\n",
      "1168/1168 [==============================] - 0s 118us/step - loss: 14882.1977 - val_loss: 31796.1955\n",
      "Epoch 643/1000\n",
      "1168/1168 [==============================] - 0s 118us/step - loss: 14772.6555 - val_loss: 32057.6112\n",
      "Epoch 644/1000\n",
      "1168/1168 [==============================] - 0s 118us/step - loss: 14628.8284 - val_loss: 31761.4132\n",
      "Epoch 645/1000\n",
      "1168/1168 [==============================] - 0s 103us/step - loss: 14847.0995 - val_loss: 31992.0561\n",
      "Epoch 646/1000\n",
      "1168/1168 [==============================] - 0s 135us/step - loss: 14817.7306 - val_loss: 31836.6872\n",
      "Epoch 647/1000\n",
      "1168/1168 [==============================] - 0s 104us/step - loss: 14743.3776 - val_loss: 31802.9087\n",
      "Epoch 648/1000\n",
      "1168/1168 [==============================] - 0s 114us/step - loss: 14759.8783 - val_loss: 32004.2869\n",
      "Epoch 649/1000\n",
      "1168/1168 [==============================] - 0s 117us/step - loss: 14780.3220 - val_loss: 31956.7832\n",
      "Epoch 650/1000\n",
      "1168/1168 [==============================] - 0s 117us/step - loss: 14586.9548 - val_loss: 31865.4109\n",
      "Epoch 651/1000\n",
      "1168/1168 [==============================] - 0s 103us/step - loss: 14883.7919 - val_loss: 31900.9978\n",
      "Epoch 652/1000\n",
      "1168/1168 [==============================] - 0s 115us/step - loss: 14571.1682 - val_loss: 31897.3680\n",
      "Epoch 653/1000\n",
      "1168/1168 [==============================] - 0s 116us/step - loss: 14840.4217 - val_loss: 31981.1721\n",
      "Epoch 654/1000\n",
      "1168/1168 [==============================] - 0s 151us/step - loss: 14699.2622 - val_loss: 31985.7509\n",
      "Epoch 655/1000\n",
      "1168/1168 [==============================] - 0s 114us/step - loss: 14454.4954 - val_loss: 32133.2351\n",
      "Epoch 656/1000\n",
      "1168/1168 [==============================] - 0s 118us/step - loss: 14723.9601 - val_loss: 31926.1716\n",
      "Epoch 657/1000\n",
      "1168/1168 [==============================] - 0s 114us/step - loss: 14523.7314 - val_loss: 32107.3409\n",
      "Epoch 658/1000\n",
      "1168/1168 [==============================] - 0s 119us/step - loss: 14605.4993 - val_loss: 31911.0588\n",
      "Epoch 659/1000\n",
      "1168/1168 [==============================] - 0s 118us/step - loss: 14648.6133 - val_loss: 31907.0254\n",
      "Epoch 660/1000\n",
      "1168/1168 [==============================] - 0s 130us/step - loss: 14630.8991 - val_loss: 32038.9409\n",
      "Epoch 661/1000\n",
      "1168/1168 [==============================] - 0s 145us/step - loss: 14613.5309 - val_loss: 32086.9569\n",
      "Epoch 662/1000\n",
      "1168/1168 [==============================] - 0s 109us/step - loss: 14392.8160 - val_loss: 32041.3863\n",
      "Epoch 663/1000\n",
      "1168/1168 [==============================] - 0s 124us/step - loss: 14655.5399 - val_loss: 31961.0101\n",
      "Epoch 664/1000\n",
      "1168/1168 [==============================] - 0s 117us/step - loss: 14762.6787 - val_loss: 31909.1114\n",
      "Epoch 665/1000\n",
      "1168/1168 [==============================] - 0s 129us/step - loss: 14630.1187 - val_loss: 31911.6201\n",
      "Epoch 666/1000\n",
      "1168/1168 [==============================] - 0s 104us/step - loss: 14602.8138 - val_loss: 31985.1695\n",
      "Epoch 667/1000\n",
      "1168/1168 [==============================] - 0s 123us/step - loss: 14743.7993 - val_loss: 32139.8655\n",
      "Epoch 668/1000\n",
      "1168/1168 [==============================] - 0s 130us/step - loss: 14500.3066 - val_loss: 32008.4659\n",
      "Epoch 669/1000\n",
      "1168/1168 [==============================] - 0s 105us/step - loss: 14455.9014 - val_loss: 32072.6882\n",
      "Epoch 670/1000\n",
      "1168/1168 [==============================] - 0s 130us/step - loss: 14581.5876 - val_loss: 32300.1223\n",
      "Epoch 671/1000\n",
      "1168/1168 [==============================] - 0s 100us/step - loss: 14652.9796 - val_loss: 31993.4320\n",
      "Epoch 672/1000\n",
      "1168/1168 [==============================] - 0s 118us/step - loss: 14555.7973 - val_loss: 32153.3007\n",
      "Epoch 673/1000\n",
      "1168/1168 [==============================] - 0s 122us/step - loss: 14605.2141 - val_loss: 32450.2105\n",
      "Epoch 674/1000\n",
      "1168/1168 [==============================] - 0s 105us/step - loss: 14729.0934 - val_loss: 32089.2063\n",
      "Epoch 675/1000\n",
      "1168/1168 [==============================] - 0s 135us/step - loss: 14691.7943 - val_loss: 32311.7849\n",
      "Epoch 676/1000\n",
      "1168/1168 [==============================] - 0s 105us/step - loss: 14644.3837 - val_loss: 32213.5589\n",
      "Epoch 677/1000\n",
      "1168/1168 [==============================] - 0s 107us/step - loss: 14663.4304 - val_loss: 32041.0397\n",
      "Epoch 678/1000\n",
      "1168/1168 [==============================] - 0s 122us/step - loss: 14599.5415 - val_loss: 32248.4670\n",
      "Epoch 679/1000\n",
      "1168/1168 [==============================] - 0s 112us/step - loss: 14544.2973 - val_loss: 32069.7505\n",
      "Epoch 680/1000\n",
      "1168/1168 [==============================] - 0s 116us/step - loss: 14452.9490 - val_loss: 32242.9365\n",
      "Epoch 681/1000\n",
      "1168/1168 [==============================] - 0s 123us/step - loss: 14669.7259 - val_loss: 32076.7958\n",
      "Epoch 682/1000\n",
      "1168/1168 [==============================] - 0s 170us/step - loss: 14388.4159 - val_loss: 32183.9432\n",
      "Epoch 683/1000\n",
      "1168/1168 [==============================] - 0s 119us/step - loss: 14347.4897 - val_loss: 32155.1995\n",
      "Epoch 684/1000\n",
      "1168/1168 [==============================] - 0s 120us/step - loss: 14429.7625 - val_loss: 32116.5498\n",
      "Epoch 685/1000\n",
      "1168/1168 [==============================] - 0s 123us/step - loss: 14273.3068 - val_loss: 32081.5720\n",
      "Epoch 686/1000\n",
      "1168/1168 [==============================] - 0s 121us/step - loss: 14354.7117 - val_loss: 32148.0795\n",
      "Epoch 687/1000\n",
      "1168/1168 [==============================] - 0s 100us/step - loss: 14750.6810 - val_loss: 32191.1172\n",
      "Epoch 688/1000\n",
      "1168/1168 [==============================] - 0s 117us/step - loss: 14427.4475 - val_loss: 32066.8635\n",
      "Epoch 689/1000\n",
      "1168/1168 [==============================] - 0s 167us/step - loss: 14376.1439 - val_loss: 32571.5884\n",
      "Epoch 690/1000\n",
      "1168/1168 [==============================] - 0s 131us/step - loss: 14261.4738 - val_loss: 32045.3993\n",
      "Epoch 691/1000\n",
      "1168/1168 [==============================] - 0s 105us/step - loss: 14497.3675 - val_loss: 32107.2395\n",
      "Epoch 692/1000\n",
      "1168/1168 [==============================] - 0s 119us/step - loss: 14254.9690 - val_loss: 32196.0886\n",
      "Epoch 693/1000\n",
      "1168/1168 [==============================] - 0s 135us/step - loss: 14435.4202 - val_loss: 32450.3199\n",
      "Epoch 694/1000\n",
      "1168/1168 [==============================] - 0s 119us/step - loss: 14544.5372 - val_loss: 32186.0671\n",
      "Epoch 695/1000\n",
      "1168/1168 [==============================] - 0s 140us/step - loss: 14247.1516 - val_loss: 32181.9881\n",
      "Epoch 696/1000\n",
      "1168/1168 [==============================] - 0s 127us/step - loss: 14341.7841 - val_loss: 32064.3548\n",
      "Epoch 697/1000\n",
      "1168/1168 [==============================] - 0s 112us/step - loss: 14366.7312 - val_loss: 32075.2791\n",
      "Epoch 698/1000\n",
      "1168/1168 [==============================] - 0s 115us/step - loss: 14272.0694 - val_loss: 32148.5843\n",
      "Epoch 699/1000\n",
      "1168/1168 [==============================] - 0s 130us/step - loss: 14376.3918 - val_loss: 32271.7123\n",
      "Epoch 700/1000\n",
      "1168/1168 [==============================] - 0s 104us/step - loss: 14427.5667 - val_loss: 32307.8041\n",
      "Epoch 701/1000\n",
      "1168/1168 [==============================] - 0s 114us/step - loss: 14382.1811 - val_loss: 32216.6644\n",
      "Epoch 702/1000\n",
      "1168/1168 [==============================] - 0s 153us/step - loss: 14344.6441 - val_loss: 32177.9089\n",
      "Epoch 703/1000\n",
      "1168/1168 [==============================] - 0s 129us/step - loss: 14388.1413 - val_loss: 32186.6602\n",
      "Epoch 704/1000\n",
      "1168/1168 [==============================] - 0s 107us/step - loss: 14351.7152 - val_loss: 32387.8929\n",
      "Epoch 705/1000\n",
      "1168/1168 [==============================] - 0s 126us/step - loss: 14400.5537 - val_loss: 32268.3060\n",
      "Epoch 706/1000\n",
      "1168/1168 [==============================] - 0s 111us/step - loss: 14273.0609 - val_loss: 32291.5547\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 707/1000\n",
      "1168/1168 [==============================] - 0s 114us/step - loss: 14309.7046 - val_loss: 32285.3377\n",
      "Epoch 708/1000\n",
      "1168/1168 [==============================] - 0s 117us/step - loss: 14324.9976 - val_loss: 32527.4450\n",
      "Epoch 709/1000\n",
      "1168/1168 [==============================] - 0s 164us/step - loss: 14154.3228 - val_loss: 32475.8013\n",
      "Epoch 710/1000\n",
      "1168/1168 [==============================] - 0s 125us/step - loss: 14127.2835 - val_loss: 32301.0489\n",
      "Epoch 711/1000\n",
      "1168/1168 [==============================] - 0s 102us/step - loss: 14370.5350 - val_loss: 32320.5813\n",
      "Epoch 712/1000\n",
      "1168/1168 [==============================] - 0s 117us/step - loss: 14414.4748 - val_loss: 32357.8508\n",
      "Epoch 713/1000\n",
      "1168/1168 [==============================] - 0s 117us/step - loss: 14004.2512 - val_loss: 32346.3487\n",
      "Epoch 714/1000\n",
      "1168/1168 [==============================] - 0s 116us/step - loss: 14371.6995 - val_loss: 32519.6115\n",
      "Epoch 715/1000\n",
      "1168/1168 [==============================] - 0s 120us/step - loss: 14450.5009 - val_loss: 32605.8870\n",
      "Epoch 716/1000\n",
      "1168/1168 [==============================] - 0s 164us/step - loss: 14261.2285 - val_loss: 32287.4214\n",
      "Epoch 717/1000\n",
      "1168/1168 [==============================] - 0s 109us/step - loss: 14135.7377 - val_loss: 32216.0452\n",
      "Epoch 718/1000\n",
      "1168/1168 [==============================] - 0s 121us/step - loss: 14213.5734 - val_loss: 32291.9365\n",
      "Epoch 719/1000\n",
      "1168/1168 [==============================] - 0s 122us/step - loss: 14274.6204 - val_loss: 33005.6015\n",
      "Epoch 720/1000\n",
      "1168/1168 [==============================] - 0s 108us/step - loss: 14070.4602 - val_loss: 32296.0651\n",
      "Epoch 721/1000\n",
      "1168/1168 [==============================] - 0s 128us/step - loss: 14183.9433 - val_loss: 32306.3422\n",
      "Epoch 722/1000\n",
      "1168/1168 [==============================] - 0s 111us/step - loss: 14033.1321 - val_loss: 32339.3041\n",
      "Epoch 723/1000\n",
      "1168/1168 [==============================] - 0s 165us/step - loss: 13975.2660 - val_loss: 32355.7880\n",
      "Epoch 724/1000\n",
      "1168/1168 [==============================] - 0s 134us/step - loss: 14186.4157 - val_loss: 32429.2999\n",
      "Epoch 725/1000\n",
      "1168/1168 [==============================] - 0s 105us/step - loss: 14301.8943 - val_loss: 32904.7195\n",
      "Epoch 726/1000\n",
      "1168/1168 [==============================] - 0s 121us/step - loss: 14394.1356 - val_loss: 32490.9158\n",
      "Epoch 727/1000\n",
      "1168/1168 [==============================] - 0s 108us/step - loss: 14182.2776 - val_loss: 32405.4836\n",
      "Epoch 728/1000\n",
      "1168/1168 [==============================] - 0s 135us/step - loss: 14218.9317 - val_loss: 32525.0746\n",
      "Epoch 729/1000\n",
      "1168/1168 [==============================] - 0s 117us/step - loss: 14239.6940 - val_loss: 32801.1920\n",
      "Epoch 730/1000\n",
      "1168/1168 [==============================] - 0s 172us/step - loss: 14086.2485 - val_loss: 32324.6845\n",
      "Epoch 731/1000\n",
      "1168/1168 [==============================] - 0s 114us/step - loss: 14021.3162 - val_loss: 32739.3042\n",
      "Epoch 732/1000\n",
      "1168/1168 [==============================] - 0s 125us/step - loss: 14039.0748 - val_loss: 32426.6765\n",
      "Epoch 733/1000\n",
      "1168/1168 [==============================] - 0s 113us/step - loss: 13923.5931 - val_loss: 32348.0114\n",
      "Epoch 734/1000\n",
      "1168/1168 [==============================] - 0s 127us/step - loss: 14088.4782 - val_loss: 32376.0993\n",
      "Epoch 735/1000\n",
      "1168/1168 [==============================] - 0s 110us/step - loss: 14223.7338 - val_loss: 32376.3625\n",
      "Epoch 736/1000\n",
      "1168/1168 [==============================] - 0s 139us/step - loss: 14013.5993 - val_loss: 32482.5120\n",
      "Epoch 737/1000\n",
      "1168/1168 [==============================] - 0s 110us/step - loss: 13903.8974 - val_loss: 32643.0603\n",
      "Epoch 738/1000\n",
      "1168/1168 [==============================] - 0s 129us/step - loss: 14053.8812 - val_loss: 32528.1567\n",
      "Epoch 739/1000\n",
      "1168/1168 [==============================] - 0s 119us/step - loss: 14153.9793 - val_loss: 32547.5274\n",
      "Epoch 740/1000\n",
      "1168/1168 [==============================] - 0s 128us/step - loss: 13967.2802 - val_loss: 32452.3950\n",
      "Epoch 741/1000\n",
      "1168/1168 [==============================] - 0s 117us/step - loss: 13788.1247 - val_loss: 32480.8636\n",
      "Epoch 742/1000\n",
      "1168/1168 [==============================] - 0s 123us/step - loss: 14046.6610 - val_loss: 32528.8152\n",
      "Epoch 743/1000\n",
      "1168/1168 [==============================] - 0s 180us/step - loss: 14058.2200 - val_loss: 32996.3823\n",
      "Epoch 744/1000\n",
      "1168/1168 [==============================] - 0s 126us/step - loss: 13956.7407 - val_loss: 32364.3546\n",
      "Epoch 745/1000\n",
      "1168/1168 [==============================] - 0s 123us/step - loss: 14096.7677 - val_loss: 32691.4813\n",
      "Epoch 746/1000\n",
      "1168/1168 [==============================] - 0s 113us/step - loss: 13937.6179 - val_loss: 32506.4310\n",
      "Epoch 747/1000\n",
      "1168/1168 [==============================] - 0s 128us/step - loss: 14054.0114 - val_loss: 32604.2423\n",
      "Epoch 748/1000\n",
      "1168/1168 [==============================] - 0s 117us/step - loss: 14013.8494 - val_loss: 32482.5504\n",
      "Epoch 749/1000\n",
      "1168/1168 [==============================] - 0s 125us/step - loss: 13876.2544 - val_loss: 32584.3065\n",
      "Epoch 750/1000\n",
      "1168/1168 [==============================] - 0s 158us/step - loss: 13905.5484 - val_loss: 32618.6543\n",
      "Epoch 751/1000\n",
      "1168/1168 [==============================] - 0s 129us/step - loss: 14078.0963 - val_loss: 32546.5145\n",
      "Epoch 752/1000\n",
      "1168/1168 [==============================] - 0s 125us/step - loss: 13870.6855 - val_loss: 32858.2594\n",
      "Epoch 753/1000\n",
      "1168/1168 [==============================] - 0s 124us/step - loss: 13857.3260 - val_loss: 32546.2834\n",
      "Epoch 754/1000\n",
      "1168/1168 [==============================] - 0s 117us/step - loss: 14127.6693 - val_loss: 32740.4074\n",
      "Epoch 755/1000\n",
      "1168/1168 [==============================] - 0s 127us/step - loss: 13819.7936 - val_loss: 32502.7844\n",
      "Epoch 756/1000\n",
      "1168/1168 [==============================] - 0s 170us/step - loss: 13842.0833 - val_loss: 32669.8918\n",
      "Epoch 757/1000\n",
      "1168/1168 [==============================] - 0s 126us/step - loss: 13898.4306 - val_loss: 32593.8121\n",
      "Epoch 758/1000\n",
      "1168/1168 [==============================] - 0s 129us/step - loss: 13975.7461 - val_loss: 32586.5900\n",
      "Epoch 759/1000\n",
      "1168/1168 [==============================] - 0s 123us/step - loss: 14077.7030 - val_loss: 32637.8107\n",
      "Epoch 760/1000\n",
      "1168/1168 [==============================] - 0s 124us/step - loss: 14113.9164 - val_loss: 32588.5988\n",
      "Epoch 761/1000\n",
      "1168/1168 [==============================] - 0s 117us/step - loss: 13869.9598 - val_loss: 32717.9128\n",
      "Epoch 762/1000\n",
      "1168/1168 [==============================] - 0s 130us/step - loss: 13908.0902 - val_loss: 32765.6356\n",
      "Epoch 763/1000\n",
      "1168/1168 [==============================] - 0s 153us/step - loss: 13930.2745 - val_loss: 32773.9849\n",
      "Epoch 764/1000\n",
      "1168/1168 [==============================] - 0s 120us/step - loss: 13875.9272 - val_loss: 32584.4969\n",
      "Epoch 765/1000\n",
      "1168/1168 [==============================] - 0s 128us/step - loss: 13921.8973 - val_loss: 32683.3238\n",
      "Epoch 766/1000\n",
      "1168/1168 [==============================] - 0s 120us/step - loss: 13862.7032 - val_loss: 32823.6174\n",
      "Epoch 767/1000\n",
      "1168/1168 [==============================] - 0s 126us/step - loss: 13933.9461 - val_loss: 32634.2224\n",
      "Epoch 768/1000\n",
      "1168/1168 [==============================] - 0s 129us/step - loss: 14059.8973 - val_loss: 32746.0376\n",
      "Epoch 769/1000\n",
      "1168/1168 [==============================] - 0s 171us/step - loss: 13662.5597 - val_loss: 32567.6221\n",
      "Epoch 770/1000\n",
      "1168/1168 [==============================] - 0s 129us/step - loss: 13732.3662 - val_loss: 32741.2955\n",
      "Epoch 771/1000\n",
      "1168/1168 [==============================] - 0s 129us/step - loss: 14007.0274 - val_loss: 32680.5576\n",
      "Epoch 772/1000\n",
      "1168/1168 [==============================] - 0s 128us/step - loss: 13836.7114 - val_loss: 32621.8262\n",
      "Epoch 773/1000\n",
      "1168/1168 [==============================] - 0s 115us/step - loss: 13848.6333 - val_loss: 32635.7307\n",
      "Epoch 774/1000\n",
      "1168/1168 [==============================] - 0s 154us/step - loss: 13893.2217 - val_loss: 32696.5880\n",
      "Epoch 775/1000\n",
      "1168/1168 [==============================] - 0s 164us/step - loss: 13883.1554 - val_loss: 32751.6918\n",
      "Epoch 776/1000\n",
      "1168/1168 [==============================] - 0s 132us/step - loss: 13959.5557 - val_loss: 32688.2758\n",
      "Epoch 777/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1168/1168 [==============================] - 0s 123us/step - loss: 13755.9430 - val_loss: 32672.8328\n",
      "Epoch 778/1000\n",
      "1168/1168 [==============================] - 0s 114us/step - loss: 13643.7157 - val_loss: 32810.0875\n",
      "Epoch 779/1000\n",
      "1168/1168 [==============================] - 0s 127us/step - loss: 13851.1124 - val_loss: 32656.1072\n",
      "Epoch 780/1000\n",
      "1168/1168 [==============================] - 0s 117us/step - loss: 13804.7152 - val_loss: 32745.8231\n",
      "Epoch 781/1000\n",
      "1168/1168 [==============================] - 0s 127us/step - loss: 13674.8333 - val_loss: 32809.3198\n",
      "Epoch 782/1000\n",
      "1168/1168 [==============================] - 0s 178us/step - loss: 13718.8264 - val_loss: 32714.1954\n",
      "Epoch 783/1000\n",
      "1168/1168 [==============================] - 0s 118us/step - loss: 13880.4346 - val_loss: 32945.5717\n",
      "Epoch 784/1000\n",
      "1168/1168 [==============================] - 0s 124us/step - loss: 13878.0260 - val_loss: 32741.6114\n",
      "Epoch 785/1000\n",
      "1168/1168 [==============================] - 0s 118us/step - loss: 13762.4363 - val_loss: 32747.7904\n",
      "Epoch 786/1000\n",
      "1168/1168 [==============================] - 0s 123us/step - loss: 13901.0478 - val_loss: 32703.0113\n",
      "Epoch 787/1000\n",
      "1168/1168 [==============================] - 0s 125us/step - loss: 13777.3482 - val_loss: 32971.9330\n",
      "Epoch 788/1000\n",
      "1168/1168 [==============================] - 0s 140us/step - loss: 13607.6526 - val_loss: 32758.7872\n",
      "Epoch 789/1000\n",
      "1168/1168 [==============================] - 0s 132us/step - loss: 13641.2718 - val_loss: 32768.2945\n",
      "Epoch 790/1000\n",
      "1168/1168 [==============================] - 0s 118us/step - loss: 13814.1872 - val_loss: 32965.4265\n",
      "Epoch 791/1000\n",
      "1168/1168 [==============================] - 0s 129us/step - loss: 13648.3341 - val_loss: 32685.9106\n",
      "Epoch 792/1000\n",
      "1168/1168 [==============================] - 0s 114us/step - loss: 13658.6131 - val_loss: 32738.7527\n",
      "Epoch 793/1000\n",
      "1168/1168 [==============================] - 0s 138us/step - loss: 13748.5300 - val_loss: 32990.6555\n",
      "Epoch 794/1000\n",
      "1168/1168 [==============================] - 0s 114us/step - loss: 13601.7942 - val_loss: 32789.8063\n",
      "Epoch 795/1000\n",
      "1168/1168 [==============================] - 0s 171us/step - loss: 13731.1444 - val_loss: 33021.8385\n",
      "Epoch 796/1000\n",
      "1168/1168 [==============================] - 0s 129us/step - loss: 13853.8400 - val_loss: 32921.1371\n",
      "Epoch 797/1000\n",
      "1168/1168 [==============================] - 0s 111us/step - loss: 13770.9616 - val_loss: 32793.3868\n",
      "Epoch 798/1000\n",
      "1168/1168 [==============================] - 0s 118us/step - loss: 13662.7833 - val_loss: 32877.3985\n",
      "Epoch 799/1000\n",
      "1168/1168 [==============================] - 0s 123us/step - loss: 13782.9091 - val_loss: 32867.4568\n",
      "Epoch 800/1000\n",
      "1168/1168 [==============================] - 0s 113us/step - loss: 13659.4277 - val_loss: 32817.6621\n",
      "Epoch 801/1000\n",
      "1168/1168 [==============================] - 0s 128us/step - loss: 13685.9539 - val_loss: 33001.6577\n",
      "Epoch 802/1000\n",
      "1168/1168 [==============================] - 0s 166us/step - loss: 13643.4162 - val_loss: 32719.8931\n",
      "Epoch 803/1000\n",
      "1168/1168 [==============================] - 0s 114us/step - loss: 13606.1969 - val_loss: 32729.1109\n",
      "Epoch 804/1000\n",
      "1168/1168 [==============================] - 0s 128us/step - loss: 13764.3250 - val_loss: 32778.0122\n",
      "Epoch 805/1000\n",
      "1168/1168 [==============================] - 0s 120us/step - loss: 13543.8761 - val_loss: 32780.9019\n",
      "Epoch 806/1000\n",
      "1168/1168 [==============================] - 0s 129us/step - loss: 13588.6797 - val_loss: 32819.1574\n",
      "Epoch 807/1000\n",
      "1168/1168 [==============================] - 0s 106us/step - loss: 13737.8309 - val_loss: 32877.0268\n",
      "Epoch 808/1000\n",
      "1168/1168 [==============================] - 0s 161us/step - loss: 13704.7301 - val_loss: 32877.4925\n",
      "Epoch 809/1000\n",
      "1168/1168 [==============================] - 0s 126us/step - loss: 13549.0103 - val_loss: 33112.5015\n",
      "Epoch 810/1000\n",
      "1168/1168 [==============================] - 0s 124us/step - loss: 13778.0974 - val_loss: 32844.2572\n",
      "Epoch 811/1000\n",
      "1168/1168 [==============================] - 0s 127us/step - loss: 13497.0322 - val_loss: 32990.3200\n",
      "Epoch 812/1000\n",
      "1168/1168 [==============================] - 0s 117us/step - loss: 13514.2660 - val_loss: 32876.0012\n",
      "Epoch 813/1000\n",
      "1168/1168 [==============================] - 0s 155us/step - loss: 13509.4537 - val_loss: 33273.3413\n",
      "Epoch 814/1000\n",
      "1168/1168 [==============================] - 0s 124us/step - loss: 13724.6355 - val_loss: 32866.3832\n",
      "Epoch 815/1000\n",
      "1168/1168 [==============================] - 0s 151us/step - loss: 13456.5027 - val_loss: 33438.2584\n",
      "Epoch 816/1000\n",
      "1168/1168 [==============================] - 0s 129us/step - loss: 13673.9601 - val_loss: 33218.5031\n",
      "Epoch 817/1000\n",
      "1168/1168 [==============================] - 0s 124us/step - loss: 13416.7931 - val_loss: 33162.0088\n",
      "Epoch 818/1000\n",
      "1168/1168 [==============================] - 0s 121us/step - loss: 13536.0928 - val_loss: 33085.1826\n",
      "Epoch 819/1000\n",
      "1168/1168 [==============================] - 0s 126us/step - loss: 13726.6897 - val_loss: 32875.1232\n",
      "Epoch 820/1000\n",
      "1168/1168 [==============================] - 0s 115us/step - loss: 13500.8430 - val_loss: 33069.3055\n",
      "Epoch 821/1000\n",
      "1168/1168 [==============================] - 0s 166us/step - loss: 13432.4456 - val_loss: 33236.3729\n",
      "Epoch 822/1000\n",
      "1168/1168 [==============================] - 0s 138us/step - loss: 13474.3246 - val_loss: 33004.9082\n",
      "Epoch 823/1000\n",
      "1168/1168 [==============================] - 0s 118us/step - loss: 13412.6390 - val_loss: 32887.6744\n",
      "Epoch 824/1000\n",
      "1168/1168 [==============================] - 0s 126us/step - loss: 13558.3185 - val_loss: 32970.7440\n",
      "Epoch 825/1000\n",
      "1168/1168 [==============================] - 0s 129us/step - loss: 13556.3107 - val_loss: 32968.6108\n",
      "Epoch 826/1000\n",
      "1168/1168 [==============================] - 0s 117us/step - loss: 13730.4349 - val_loss: 32983.1012\n",
      "Epoch 827/1000\n",
      "1168/1168 [==============================] - 0s 134us/step - loss: 13594.0751 - val_loss: 33288.5444\n",
      "Epoch 828/1000\n",
      "1168/1168 [==============================] - 0s 145us/step - loss: 13657.1621 - val_loss: 33068.2412\n",
      "Epoch 829/1000\n",
      "1168/1168 [==============================] - 0s 131us/step - loss: 13461.1862 - val_loss: 32997.6701\n",
      "Epoch 830/1000\n",
      "1168/1168 [==============================] - 0s 112us/step - loss: 13473.1948 - val_loss: 33077.3547\n",
      "Epoch 831/1000\n",
      "1168/1168 [==============================] - 0s 135us/step - loss: 13504.3014 - val_loss: 32911.7894\n",
      "Epoch 832/1000\n",
      "1168/1168 [==============================] - 0s 129us/step - loss: 13380.1374 - val_loss: 32933.0597\n",
      "Epoch 833/1000\n",
      "1168/1168 [==============================] - 0s 123us/step - loss: 13497.2041 - val_loss: 33035.8723\n",
      "Epoch 834/1000\n",
      "1168/1168 [==============================] - 0s 159us/step - loss: 13342.8725 - val_loss: 33294.7954\n",
      "Epoch 835/1000\n",
      "1168/1168 [==============================] - 0s 150us/step - loss: 13235.4238 - val_loss: 32945.9177\n",
      "Epoch 836/1000\n",
      "1168/1168 [==============================] - 0s 134us/step - loss: 13363.5629 - val_loss: 33100.5493\n",
      "Epoch 837/1000\n",
      "1168/1168 [==============================] - 0s 122us/step - loss: 13474.5546 - val_loss: 32976.0168\n",
      "Epoch 838/1000\n",
      "1168/1168 [==============================] - 0s 129us/step - loss: 13507.8578 - val_loss: 33158.3721\n",
      "Epoch 839/1000\n",
      "1168/1168 [==============================] - 0s 127us/step - loss: 13446.8510 - val_loss: 33011.0719\n",
      "Epoch 840/1000\n",
      "1168/1168 [==============================] - 0s 143us/step - loss: 13455.6967 - val_loss: 32997.7557\n",
      "Epoch 841/1000\n",
      "1168/1168 [==============================] - 0s 147us/step - loss: 13300.8366 - val_loss: 32978.4105\n",
      "Epoch 842/1000\n",
      "1168/1168 [==============================] - 0s 128us/step - loss: 13492.3189 - val_loss: 32965.8655\n",
      "Epoch 843/1000\n",
      "1168/1168 [==============================] - 0s 140us/step - loss: 13460.5232 - val_loss: 33012.9339\n",
      "Epoch 844/1000\n",
      "1168/1168 [==============================] - 0s 115us/step - loss: 13484.6765 - val_loss: 33081.2759\n",
      "Epoch 845/1000\n",
      "1168/1168 [==============================] - 0s 155us/step - loss: 13359.2335 - val_loss: 33081.8267\n",
      "Epoch 846/1000\n",
      "1168/1168 [==============================] - 0s 144us/step - loss: 13363.1267 - val_loss: 33034.8719\n",
      "Epoch 847/1000\n",
      "1168/1168 [==============================] - 0s 149us/step - loss: 13541.4169 - val_loss: 33040.3764\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 848/1000\n",
      "1168/1168 [==============================] - 0s 123us/step - loss: 13420.0659 - val_loss: 33043.3282\n",
      "Epoch 849/1000\n",
      "1168/1168 [==============================] - 0s 114us/step - loss: 13259.5315 - val_loss: 33051.8878\n",
      "Epoch 850/1000\n",
      "1168/1168 [==============================] - 0s 125us/step - loss: 13108.5035 - val_loss: 33073.6000\n",
      "Epoch 851/1000\n",
      "1168/1168 [==============================] - 0s 114us/step - loss: 13272.3658 - val_loss: 33254.5372\n",
      "Epoch 852/1000\n",
      "1168/1168 [==============================] - 0s 126us/step - loss: 13469.6235 - val_loss: 33131.8942\n",
      "Epoch 853/1000\n",
      "1168/1168 [==============================] - 0s 140us/step - loss: 13301.7870 - val_loss: 33101.3538\n",
      "Epoch 854/1000\n",
      "1168/1168 [==============================] - 0s 125us/step - loss: 13407.2115 - val_loss: 33286.4928\n",
      "Epoch 855/1000\n",
      "1168/1168 [==============================] - 0s 122us/step - loss: 13451.8988 - val_loss: 33073.8056\n",
      "Epoch 856/1000\n",
      "1168/1168 [==============================] - 0s 114us/step - loss: 13440.5377 - val_loss: 32990.7005\n",
      "Epoch 857/1000\n",
      "1168/1168 [==============================] - 0s 126us/step - loss: 13446.5156 - val_loss: 33111.8816\n",
      "Epoch 858/1000\n",
      "1168/1168 [==============================] - 0s 114us/step - loss: 13254.8977 - val_loss: 33243.5064\n",
      "Epoch 859/1000\n",
      "1168/1168 [==============================] - 0s 127us/step - loss: 13411.4383 - val_loss: 33164.6411\n",
      "Epoch 860/1000\n",
      "1168/1168 [==============================] - 0s 164us/step - loss: 13310.4291 - val_loss: 33159.4240\n",
      "Epoch 861/1000\n",
      "1168/1168 [==============================] - 0s 129us/step - loss: 13377.0149 - val_loss: 33203.5962\n",
      "Epoch 862/1000\n",
      "1168/1168 [==============================] - 0s 113us/step - loss: 13314.9179 - val_loss: 33070.7932\n",
      "Epoch 863/1000\n",
      "1168/1168 [==============================] - 0s 131us/step - loss: 13336.9610 - val_loss: 33166.7913\n",
      "Epoch 864/1000\n",
      "1168/1168 [==============================] - 0s 117us/step - loss: 13533.9280 - val_loss: 33150.9373\n",
      "Epoch 865/1000\n",
      "1168/1168 [==============================] - 0s 127us/step - loss: 13144.2956 - val_loss: 33527.0164\n",
      "Epoch 866/1000\n",
      "1168/1168 [==============================] - 0s 127us/step - loss: 13416.3746 - val_loss: 33150.3776\n",
      "Epoch 867/1000\n",
      "1168/1168 [==============================] - 0s 147us/step - loss: 13324.7611 - val_loss: 33116.9937\n",
      "Epoch 868/1000\n",
      "1168/1168 [==============================] - 0s 123us/step - loss: 13197.0311 - val_loss: 33164.5894\n",
      "Epoch 869/1000\n",
      "1168/1168 [==============================] - 0s 115us/step - loss: 13387.2268 - val_loss: 33171.8158\n",
      "Epoch 870/1000\n",
      "1168/1168 [==============================] - 0s 125us/step - loss: 13443.1926 - val_loss: 33194.2435\n",
      "Epoch 871/1000\n",
      "1168/1168 [==============================] - 0s 117us/step - loss: 13202.2033 - val_loss: 33169.7573\n",
      "Epoch 872/1000\n",
      "1168/1168 [==============================] - 0s 128us/step - loss: 13198.9441 - val_loss: 33231.5923\n",
      "Epoch 873/1000\n",
      "1168/1168 [==============================] - 0s 141us/step - loss: 13373.1893 - val_loss: 33165.5381\n",
      "Epoch 874/1000\n",
      "1168/1168 [==============================] - 0s 125us/step - loss: 13310.3638 - val_loss: 33144.1474\n",
      "Epoch 875/1000\n",
      "1168/1168 [==============================] - 0s 120us/step - loss: 13185.4262 - val_loss: 33218.1097\n",
      "Epoch 876/1000\n",
      "1168/1168 [==============================] - 0s 122us/step - loss: 13095.0048 - val_loss: 33305.5762\n",
      "Epoch 877/1000\n",
      "1168/1168 [==============================] - 0s 122us/step - loss: 13164.2353 - val_loss: 33248.1408\n",
      "Epoch 878/1000\n",
      "1168/1168 [==============================] - 0s 123us/step - loss: 13211.4400 - val_loss: 33208.3875\n",
      "Epoch 879/1000\n",
      "1168/1168 [==============================] - 0s 129us/step - loss: 13297.4718 - val_loss: 33228.9913\n",
      "Epoch 880/1000\n",
      "1168/1168 [==============================] - 0s 174us/step - loss: 13171.0208 - val_loss: 33357.2120\n",
      "Epoch 881/1000\n",
      "1168/1168 [==============================] - 0s 130us/step - loss: 13272.1893 - val_loss: 33387.2665\n",
      "Epoch 882/1000\n",
      "1168/1168 [==============================] - 0s 115us/step - loss: 13220.2248 - val_loss: 33244.4690\n",
      "Epoch 883/1000\n",
      "1168/1168 [==============================] - 0s 126us/step - loss: 13210.0602 - val_loss: 33233.0501\n",
      "Epoch 884/1000\n",
      "1168/1168 [==============================] - 0s 111us/step - loss: 12994.8305 - val_loss: 33493.9362\n",
      "Epoch 885/1000\n",
      "1168/1168 [==============================] - 0s 129us/step - loss: 13127.7200 - val_loss: 33412.0639\n",
      "Epoch 886/1000\n",
      "1168/1168 [==============================] - 0s 110us/step - loss: 12942.0583 - val_loss: 33277.0648\n",
      "Epoch 887/1000\n",
      "1168/1168 [==============================] - 0s 162us/step - loss: 13433.9016 - val_loss: 33431.9504\n",
      "Epoch 888/1000\n",
      "1168/1168 [==============================] - 0s 130us/step - loss: 13037.5821 - val_loss: 33206.6237\n",
      "Epoch 889/1000\n",
      "1168/1168 [==============================] - 0s 125us/step - loss: 13226.7934 - val_loss: 33257.7226\n",
      "Epoch 890/1000\n",
      "1168/1168 [==============================] - 0s 130us/step - loss: 13182.4528 - val_loss: 33388.1919\n",
      "Epoch 891/1000\n",
      "1168/1168 [==============================] - 0s 124us/step - loss: 13018.1419 - val_loss: 33295.8524\n",
      "Epoch 892/1000\n",
      "1168/1168 [==============================] - 0s 125us/step - loss: 13146.6338 - val_loss: 33366.4819\n",
      "Epoch 893/1000\n",
      "1168/1168 [==============================] - 0s 158us/step - loss: 13018.9310 - val_loss: 34024.6823\n",
      "Epoch 894/1000\n",
      "1168/1168 [==============================] - 0s 146us/step - loss: 13225.4718 - val_loss: 33219.6705\n",
      "Epoch 895/1000\n",
      "1168/1168 [==============================] - 0s 134us/step - loss: 13259.7249 - val_loss: 33222.5039\n",
      "Epoch 896/1000\n",
      "1168/1168 [==============================] - 0s 129us/step - loss: 13155.3195 - val_loss: 33336.2544\n",
      "Epoch 897/1000\n",
      "1168/1168 [==============================] - 0s 128us/step - loss: 13020.6191 - val_loss: 33210.0465\n",
      "Epoch 898/1000\n",
      "1168/1168 [==============================] - 0s 117us/step - loss: 13135.9145 - val_loss: 33225.8627\n",
      "Epoch 899/1000\n",
      "1168/1168 [==============================] - 0s 128us/step - loss: 13133.0146 - val_loss: 33408.9987\n",
      "Epoch 900/1000\n",
      "1168/1168 [==============================] - 0s 168us/step - loss: 13027.5766 - val_loss: 33287.0260\n",
      "Epoch 901/1000\n",
      "1168/1168 [==============================] - 0s 114us/step - loss: 13336.3586 - val_loss: 33703.3249\n",
      "Epoch 902/1000\n",
      "1168/1168 [==============================] - 0s 129us/step - loss: 13249.4578 - val_loss: 33497.4196\n",
      "Epoch 903/1000\n",
      "1168/1168 [==============================] - 0s 121us/step - loss: 13288.5278 - val_loss: 33556.4303\n",
      "Epoch 904/1000\n",
      "1168/1168 [==============================] - 0s 129us/step - loss: 13153.4742 - val_loss: 33494.9062\n",
      "Epoch 905/1000\n",
      "1168/1168 [==============================] - 0s 122us/step - loss: 12860.6833 - val_loss: 33441.3035\n",
      "Epoch 906/1000\n",
      "1168/1168 [==============================] - 0s 149us/step - loss: 13046.0843 - val_loss: 33387.5822\n",
      "Epoch 907/1000\n",
      "1168/1168 [==============================] - 0s 126us/step - loss: 13031.0556 - val_loss: 33563.5608\n",
      "Epoch 908/1000\n",
      "1168/1168 [==============================] - 0s 111us/step - loss: 13096.7781 - val_loss: 33421.5458\n",
      "Epoch 909/1000\n",
      "1168/1168 [==============================] - 0s 146us/step - loss: 13018.5465 - val_loss: 33651.2632\n",
      "Epoch 910/1000\n",
      "1168/1168 [==============================] - 0s 160us/step - loss: 13304.7432 - val_loss: 33298.3900\n",
      "Epoch 911/1000\n",
      "1168/1168 [==============================] - 0s 168us/step - loss: 13060.8859 - val_loss: 33366.1671\n",
      "Epoch 912/1000\n",
      "1168/1168 [==============================] - 0s 179us/step - loss: 12947.2776 - val_loss: 33322.8681\n",
      "Epoch 913/1000\n",
      "1168/1168 [==============================] - 0s 153us/step - loss: 13135.6190 - val_loss: 33427.7866\n",
      "Epoch 914/1000\n",
      "1168/1168 [==============================] - 0s 176us/step - loss: 13212.9917 - val_loss: 33334.3785\n",
      "Epoch 915/1000\n",
      "1168/1168 [==============================] - 0s 168us/step - loss: 13143.9057 - val_loss: 33399.4968\n",
      "Epoch 916/1000\n",
      "1168/1168 [==============================] - 0s 129us/step - loss: 12975.2117 - val_loss: 33474.1768\n",
      "Epoch 917/1000\n",
      "1168/1168 [==============================] - 0s 160us/step - loss: 13006.6568 - val_loss: 33673.5199\n",
      "Epoch 918/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1168/1168 [==============================] - 0s 151us/step - loss: 13051.7057 - val_loss: 33447.9571\n",
      "Epoch 919/1000\n",
      "1168/1168 [==============================] - 0s 120us/step - loss: 12926.3757 - val_loss: 33551.1971\n",
      "Epoch 920/1000\n",
      "1168/1168 [==============================] - 0s 127us/step - loss: 13114.8578 - val_loss: 33364.3740\n",
      "Epoch 921/1000\n",
      "1168/1168 [==============================] - 0s 124us/step - loss: 12879.0903 - val_loss: 33565.8788\n",
      "Epoch 922/1000\n",
      "1168/1168 [==============================] - 0s 128us/step - loss: 12975.9697 - val_loss: 33430.5096\n",
      "Epoch 923/1000\n",
      "1168/1168 [==============================] - 0s 111us/step - loss: 12979.7289 - val_loss: 33435.7486\n",
      "Epoch 924/1000\n",
      "1168/1168 [==============================] - 0s 137us/step - loss: 13128.0465 - val_loss: 33374.1163\n",
      "Epoch 925/1000\n",
      "1168/1168 [==============================] - 0s 113us/step - loss: 13054.7521 - val_loss: 33593.1167\n",
      "Epoch 926/1000\n",
      "1168/1168 [==============================] - 0s 125us/step - loss: 13044.3685 - val_loss: 33510.7034\n",
      "Epoch 927/1000\n",
      "1168/1168 [==============================] - 0s 118us/step - loss: 13020.0335 - val_loss: 33444.7400\n",
      "Epoch 928/1000\n",
      "1168/1168 [==============================] - 0s 123us/step - loss: 13013.0042 - val_loss: 33484.4816\n",
      "Epoch 929/1000\n",
      "1168/1168 [==============================] - 0s 113us/step - loss: 13001.1880 - val_loss: 33639.9538\n",
      "Epoch 930/1000\n",
      "1168/1168 [==============================] - 0s 148us/step - loss: 13010.8582 - val_loss: 33479.6485\n",
      "Epoch 931/1000\n",
      "1168/1168 [==============================] - 0s 128us/step - loss: 12825.2242 - val_loss: 33501.4754\n",
      "Epoch 932/1000\n",
      "1168/1168 [==============================] - ETA: 0s - loss: 11664.749 - 0s 112us/step - loss: 12933.6116 - val_loss: 33422.0978\n",
      "Epoch 933/1000\n",
      "1168/1168 [==============================] - 0s 129us/step - loss: 13036.0867 - val_loss: 33473.1529\n",
      "Epoch 934/1000\n",
      "1168/1168 [==============================] - 0s 117us/step - loss: 12874.4403 - val_loss: 33618.7518\n",
      "Epoch 935/1000\n",
      "1168/1168 [==============================] - 0s 128us/step - loss: 12952.3034 - val_loss: 33570.3257\n",
      "Epoch 936/1000\n",
      "1168/1168 [==============================] - 0s 115us/step - loss: 12909.0377 - val_loss: 33437.3094\n",
      "Epoch 937/1000\n",
      "1168/1168 [==============================] - 0s 163us/step - loss: 12897.4920 - val_loss: 33746.6934\n",
      "Epoch 938/1000\n",
      "1168/1168 [==============================] - 0s 128us/step - loss: 12907.0031 - val_loss: 33521.7642\n",
      "Epoch 939/1000\n",
      "1168/1168 [==============================] - 0s 110us/step - loss: 12781.9706 - val_loss: 33552.9455\n",
      "Epoch 940/1000\n",
      "1168/1168 [==============================] - 0s 127us/step - loss: 13044.7346 - val_loss: 33438.1382\n",
      "Epoch 941/1000\n",
      "1168/1168 [==============================] - 0s 117us/step - loss: 12922.1105 - val_loss: 33500.2791\n",
      "Epoch 942/1000\n",
      "1168/1168 [==============================] - 0s 130us/step - loss: 13016.8949 - val_loss: 33727.5892\n",
      "Epoch 943/1000\n",
      "1168/1168 [==============================] - 0s 115us/step - loss: 12825.1750 - val_loss: 33714.1244\n",
      "Epoch 944/1000\n",
      "1168/1168 [==============================] - 0s 169us/step - loss: 12934.3310 - val_loss: 33673.7997\n",
      "Epoch 945/1000\n",
      "1168/1168 [==============================] - 0s 124us/step - loss: 12781.8182 - val_loss: 33566.9389\n",
      "Epoch 946/1000\n",
      "1168/1168 [==============================] - 0s 129us/step - loss: 12902.9001 - val_loss: 33715.6998\n",
      "Epoch 947/1000\n",
      "1168/1168 [==============================] - 0s 121us/step - loss: 13070.5148 - val_loss: 33619.7802\n",
      "Epoch 948/1000\n",
      "1168/1168 [==============================] - 0s 126us/step - loss: 12840.7683 - val_loss: 33572.5226\n",
      "Epoch 949/1000\n",
      "1168/1168 [==============================] - 0s 120us/step - loss: 12983.5467 - val_loss: 33646.7100\n",
      "Epoch 950/1000\n",
      "1168/1168 [==============================] - 0s 140us/step - loss: 12921.8862 - val_loss: 33521.2785\n",
      "Epoch 951/1000\n",
      "1168/1168 [==============================] - 0s 126us/step - loss: 12885.5684 - val_loss: 33632.2104\n",
      "Epoch 952/1000\n",
      "1168/1168 [==============================] - 0s 130us/step - loss: 12871.3226 - val_loss: 33561.0862\n",
      "Epoch 953/1000\n",
      "1168/1168 [==============================] - 0s 112us/step - loss: 12949.6040 - val_loss: 33527.2958\n",
      "Epoch 954/1000\n",
      "1168/1168 [==============================] - 0s 127us/step - loss: 12682.4752 - val_loss: 33801.0298\n",
      "Epoch 955/1000\n",
      "1168/1168 [==============================] - 0s 123us/step - loss: 12763.9311 - val_loss: 33574.2661\n",
      "Epoch 956/1000\n",
      "1168/1168 [==============================] - 0s 131us/step - loss: 12795.3172 - val_loss: 33794.1121\n",
      "Epoch 957/1000\n",
      "1168/1168 [==============================] - 0s 181us/step - loss: 12732.2638 - val_loss: 33530.0549\n",
      "Epoch 958/1000\n",
      "1168/1168 [==============================] - 0s 119us/step - loss: 12982.7151 - val_loss: 33667.1360\n",
      "Epoch 959/1000\n",
      "1168/1168 [==============================] - 0s 132us/step - loss: 12876.8203 - val_loss: 33726.8545\n",
      "Epoch 960/1000\n",
      "1168/1168 [==============================] - 0s 128us/step - loss: 12892.8900 - val_loss: 33600.3847\n",
      "Epoch 961/1000\n",
      "1168/1168 [==============================] - 0s 113us/step - loss: 12916.6428 - val_loss: 33661.1739\n",
      "Epoch 962/1000\n",
      "1168/1168 [==============================] - 0s 134us/step - loss: 12775.1796 - val_loss: 33981.7962\n",
      "Epoch 963/1000\n",
      "1168/1168 [==============================] - 0s 129us/step - loss: 12899.6986 - val_loss: 33836.7140\n",
      "Epoch 964/1000\n",
      "1168/1168 [==============================] - 0s 129us/step - loss: 12908.4590 - val_loss: 33646.8834\n",
      "Epoch 965/1000\n",
      "1168/1168 [==============================] - 0s 129us/step - loss: 12702.8996 - val_loss: 33638.8161\n",
      "Epoch 966/1000\n",
      "1168/1168 [==============================] - 0s 126us/step - loss: 12707.3281 - val_loss: 33582.9062\n",
      "Epoch 967/1000\n",
      "1168/1168 [==============================] - 0s 119us/step - loss: 12963.3927 - val_loss: 33567.2694\n",
      "Epoch 968/1000\n",
      "1168/1168 [==============================] - 0s 128us/step - loss: 12681.8754 - val_loss: 33603.2845\n",
      "Epoch 969/1000\n",
      "1168/1168 [==============================] - 0s 130us/step - loss: 12901.0227 - val_loss: 33806.7906\n",
      "Epoch 970/1000\n",
      "1168/1168 [==============================] - 0s 164us/step - loss: 13000.0884 - val_loss: 33643.1579\n",
      "Epoch 971/1000\n",
      "1168/1168 [==============================] - 0s 130us/step - loss: 12692.1551 - val_loss: 33805.8073\n",
      "Epoch 972/1000\n",
      "1168/1168 [==============================] - 0s 125us/step - loss: 12697.3010 - val_loss: 33673.0819\n",
      "Epoch 973/1000\n",
      "1168/1168 [==============================] - 0s 115us/step - loss: 12782.8144 - val_loss: 33811.3091\n",
      "Epoch 974/1000\n",
      "1168/1168 [==============================] - 0s 129us/step - loss: 12891.8099 - val_loss: 33683.4740\n",
      "Epoch 975/1000\n",
      "1168/1168 [==============================] - 0s 134us/step - loss: 12701.7887 - val_loss: 34052.1480\n",
      "Epoch 976/1000\n",
      "1168/1168 [==============================] - 0s 157us/step - loss: 12751.8066 - val_loss: 33698.6098\n",
      "Epoch 977/1000\n",
      "1168/1168 [==============================] - 0s 165us/step - loss: 12558.7452 - val_loss: 33925.3961\n",
      "Epoch 978/1000\n",
      "1168/1168 [==============================] - 0s 137us/step - loss: 12869.1735 - val_loss: 33684.1593\n",
      "Epoch 979/1000\n",
      "1168/1168 [==============================] - 0s 118us/step - loss: 12761.2677 - val_loss: 33678.3824\n",
      "Epoch 980/1000\n",
      "1168/1168 [==============================] - 0s 127us/step - loss: 12791.6520 - val_loss: 33942.3506\n",
      "Epoch 981/1000\n",
      "1168/1168 [==============================] - 0s 115us/step - loss: 12448.8537 - val_loss: 33724.0858\n",
      "Epoch 982/1000\n",
      "1168/1168 [==============================] - 0s 130us/step - loss: 12655.1409 - val_loss: 33735.7369\n",
      "Epoch 983/1000\n",
      "1168/1168 [==============================] - 0s 159us/step - loss: 12656.0190 - val_loss: 33859.1962\n",
      "Epoch 984/1000\n",
      "1168/1168 [==============================] - 0s 128us/step - loss: 12926.6928 - val_loss: 33709.4915\n",
      "Epoch 985/1000\n",
      "1168/1168 [==============================] - 0s 114us/step - loss: 12736.6366 - val_loss: 33667.5801\n",
      "Epoch 986/1000\n",
      "1168/1168 [==============================] - 0s 138us/step - loss: 12628.4624 - val_loss: 33877.9927\n",
      "Epoch 987/1000\n",
      "1168/1168 [==============================] - 0s 119us/step - loss: 12692.6321 - val_loss: 33736.3734\n",
      "Epoch 988/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1168/1168 [==============================] - 0s 132us/step - loss: 12705.9403 - val_loss: 33779.8412\n",
      "Epoch 989/1000\n",
      "1168/1168 [==============================] - 0s 154us/step - loss: 12777.2727 - val_loss: 33986.2848\n",
      "Epoch 990/1000\n",
      "1168/1168 [==============================] - 0s 125us/step - loss: 12720.1163 - val_loss: 33803.2626\n",
      "Epoch 991/1000\n",
      "1168/1168 [==============================] - 0s 116us/step - loss: 12774.8619 - val_loss: 33850.9884\n",
      "Epoch 992/1000\n",
      "1168/1168 [==============================] - 0s 127us/step - loss: 12752.2838 - val_loss: 33840.2360\n",
      "Epoch 993/1000\n",
      "1168/1168 [==============================] - 0s 114us/step - loss: 12684.6215 - val_loss: 33840.0427\n",
      "Epoch 994/1000\n",
      "1168/1168 [==============================] - 0s 126us/step - loss: 12832.3337 - val_loss: 33786.8988\n",
      "Epoch 995/1000\n",
      "1168/1168 [==============================] - 0s 117us/step - loss: 12631.1196 - val_loss: 34001.1189\n",
      "Epoch 996/1000\n",
      "1168/1168 [==============================] - 0s 179us/step - loss: 12701.0298 - val_loss: 33733.6924\n",
      "Epoch 997/1000\n",
      "1168/1168 [==============================] - 0s 119us/step - loss: 12618.5771 - val_loss: 33817.8433\n",
      "Epoch 998/1000\n",
      "1168/1168 [==============================] - 0s 124us/step - loss: 12688.7761 - val_loss: 33774.7400\n",
      "Epoch 999/1000\n",
      "1168/1168 [==============================] - 0s 129us/step - loss: 12632.7327 - val_loss: 34098.1904\n",
      "Epoch 1000/1000\n",
      "1168/1168 [==============================] - 0s 114us/step - loss: 12678.0579 - val_loss: 33803.1608\n"
     ]
    }
   ],
   "source": [
    "Neural_model  = Sequential()\n",
    "\n",
    "Neural_model.add(Dense(output_dim = 50, init='he_uniform',activation='relu',input_dim=158))\n",
    "\n",
    "Neural_model.add(Dense(output_dim = 25, init='he_uniform',activation='relu'))\n",
    "\n",
    "Neural_model.add(Dense(output_dim = 50, init='he_uniform',activation='relu'))\n",
    "\n",
    "Neural_model.add(Dense(output_dim = 1,init ='he_uniform'))\n",
    "\n",
    "Neural_model.compile(loss=root_mean_squared_error,optimizer='Adamax')\n",
    "\n",
    "model_history = Neural_model.fit(X.values,Y.values,validation_split=0.20,batch_size=10,nb_epoch = 1000)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {},
   "outputs": [],
   "source": [
    "ann_pred= Neural_model.predict(new_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras import backend as K\n",
    "def root_mean_squared_error(y_true,y_pred):\n",
    "    return K.sqrt(K.mean(K.square(y_pred - y_true)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {},
   "outputs": [],
   "source": [
    "ypred_neural = pd.DataFrame(ann_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_test= pd.read_csv(\"D:\\Data scientist\\Dataset\\house-prices-advanced-regression-techniques\\sample_submission_default.csv\")\n",
    "datasets2 = pd.concat([y_test['Id'],ypred_neural],axis=1)\n",
    "datasets2.columns=['Id','SalePrice']\n",
    "datasets2.to_csv('D:\\Data scientist\\Dataset\\house-prices-advanced-regression-techniques\\sample_submission.csv',index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
